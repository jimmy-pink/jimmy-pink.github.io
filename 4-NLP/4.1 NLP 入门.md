
## 什么是NLP?
> **NLP**（自然语言处理，Natural Language Processing）, 致力于让计算机能够理解、解释和生成人类语言(自然语言)。

- 核心目标：
	- **语言理解**：使计算机理解人类语言的含义和语境。例如：
		- 语义理解
		- 情感分析
		- 实体识别
	- **语言生成**：使计算机能够生成合乎逻辑的文本或语言。例如：
		- 机器翻译
		- 文本摘要
		- 聊天机器人


## 核心任务
- 词法分析 Lexical Analysis
	- 分词 Tokenization
	- 词性标注 Part-of-Speech Tagging
	- 词汇岐义消解 Word Sense Disambiguation
	- 命名实体识别 Named Entity Recognition
	- 拼写纠正 Spelling Correction
- 句法分析 Syntactic Analysis
	- 依存句法分析 Dependency Parsing
	- 成分句法分析 Constituency Parsing
	- 句法纠错 Syntactic Error Correction
	- 语法生成 Grammar Generation
- 语义理解 Semantic Understanding
	- 关系抽取 Relation Extraction
	- 事件抽取 Event Extraction
	- 情感分析 Sentiment Analysis
	- 文本蕴含 Textual Entailment
	- 共指消解 Coreference Resolution
	- 语义角色标 Semantic Role Labeling
- 语用分析 Pragmatics Analysis
	- 对话管理 Dialogue Management
	- 言外之意推理 Implicature Inference
	- 语言生成 Language Generation
	- 证据识别 Tone Detection
	- 推理与共识 Inferences and Consensus

### 词法分析
#### 分词 Tokenization
- 定义：
	- 将一段文本拆分成更小的单元 (Token)，通常是单词、字符或子词。
- 分词方法
	- 基于空格的分词
	- 基于规则的分词
	- 基于词典的分词
		- 使用预先训练好的词典来判断如何拆分单词，如es的IK分词器


<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">子词分词</font>
![[sub-tokenization.png]]
- 定义
	- 比word tokenization更精细的分词方法
	- 把word拆分成更少的单元，如，词根，词缀等
- 目的：
	- 处理 未登录词 (Out-of-Vocabulary)
- 好处
	- 减少词汇量
	- 解决未登录词问题
- 常见的子词分词方法：
	- Byte Pair Encoding (BPE)
		- BPE 将高频的字符对（如 “l” 和 “y”）合并成一个新单词，从而减少词汇量
	- WordPiece
		- 类似于BPE，但通过最大化似然估计来学习最佳的子词词典
	- SentencePiece
		- 与WordPiece类似，是一个独立的分词模型，可直接从未分词的文本中学习子词。
	
<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">WordPiece</font>
- WordPiece 是一种重要的子词分词方法。
- 工作原理：
	1. 从文本中抽取所有字符，计算频率
	2. 使用 **最大似然估计** 来合并频率最高的字符对。
	3. 反复进行字符对合并，直到满足预设的词汇表大小，或者无法再合并频率较高的字符对。
- 特点：
	- 动态词汇表
	- 处理未登录词
![[WordPiece.png]]

### 语义理解
#### 情感分析 Semantic Analysis
 
- 定义
	- 判断文本的情感倾向，是正面、负面还是中立
- 🌰 **imdb_reviews 影评情感分析**  
	- [✍️ Github-imdb_reviews-rnn](https://github.com/jimmy-pink/colab-machinelearning-playground/blob/main/tensorflow/nlp-imdb_reviews-rnn.ipynb)

## 特征表示

### **矢量化（Vectorization）**
- 定义：
	- 将**文本数据转化为数值向量**的过程
- 主要矢量方法
	- 词袋模型 Bag-of-Words BoW
		- 文本被表示为一个稀疏向量
		- 向量中每个元素表示词汇表中某个单词在文档中的**词频** 或 **TF-IDF**
	- n-gram
		- 将文本分割成n个连续的词 组成的序列。
		- 是词袋模式的改进，通过考虑词序来捕捉更多上下文信息
	- 独热编码 One-hot Encoding
		- 将每个词转化为一个词汇表大小的稀疏向量，表示一个词在词汇表中的存在与否。
	- Word Embedding：
		- 现代特征表示

### 词向量法 Word Embedding
![[word-embedding-full.png]]
上图是Token 的8维词向量表示。  
每个数值表示 一个**单词** Word 在**特定维度** (Dimension) 上的嵌入值。

- 定义
	- 将单词映射到**低维向量空间**的技术
	- 是矢量化的一种方法
	- 也叫 **词向量法** 
- 目标：
	- 将每个词表示为一个稠密向量
	- 这个向量能够有效地捕捉该词的语义和上下文信息
- 特征：
	- 稠密向量
		- 向量中大多数值都不是0
	- 语义相似性
		- 相似的词在向量空间中会靠得很近
	- 低维表示
		- 将单词映射到相对低维空间 如100-300维
- Embedding Projector 词向量映射
	- [Tensorflow Embedding Projector](https://projector.tensorflow.org/)
#### Word2Vec
- 定义
	- **基于上下文**的Word Embedding
- 核心思想
	- 依赖局部上下文窗口，捕捉words之意的关系
- 训练：
	- **CBOW** (Continuous Bag of Words)
		- 根据上下文词频预测目标词
	- **Skip-Gram**
		- 与CBOW相反，Skip-Gram根据目标词来预测上下文的词，即给定目标词，预测上下文。
#### GloVe
- 定义：
	- **基于矩阵分解**的Word Embedding
- 核心思想：
	- 通过 **词频统计** (如共现矩阵) 来捕捉Words之意的关系
		- GloVe 会构建一个共现矩阵，记录在整个语料库中，每对词同时出现的次数。
		- 然后对这个矩阵分解，学习到一个低维度的词向量表示
- glove.6B.100d.txt
	- 预训练60亿参数的 GloVe（Global Vectors for Word Representation）
	- 包含了 400,000 个单词（或子词）及其对应的 100维 的词向量。

### 上下文向量

#### BERT
