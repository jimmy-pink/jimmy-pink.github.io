<h1 style=" text-align: center; font-size: 3em; font-family: 'Georgia', serif; color: #2c3e50; margin: 0.5em 0; padding: 10px 0; border-top: 4px solid #3498db; border-bottom: 4px solid #3498db; text-transform: uppercase; letter-spacing: 3px;"> 监督学习常用算法</h1>


## 回归 (Regression)
![[regression@2.webp]]

<font style="background-color: salmon; color:black">The process of going back to an earlier or less advanced form or state.</font>

<font style="background-color:yellow; color:black">回归 = 找出一个“输入”和“数值型输出”之间的规律，建立预测公式。</font>

- 定义：用于<font style="background-color:salmon; color:black">研究变量之间关系</font>的统计方法
- 🎯 目标：
	- 总结函数：理解自变量对因变量的影响程度和方向
	- 用于预测：预测在给定自变量值的情况下，因变量的可能取值
	
- 🌰：用回归算法尝试估计 $y=2x$ 这个函数，然后预测x=n，y=？
- 公式：$$y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_n X_{ki} + \varepsilon$$
	- y 是因变量
	- `x₁, x₂, ..., xₙ` 是自变量 
	- `β₀, β₁, β₂, ..., βₙ` 是模型参数（也称为回归系数）
	- ε 是==误差项==，代表模型无法解释的随机变异
- 评估：
	- 拟合优度 $R^2$  
- 分类1：
	- 线性回归 （Simple  Regression）
		- 只包含**一个变量**$$y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$
	- 多元回归 (Multiple  Regression)
		- 包含 **多个自变量**，**逐步控制、逐个研究**$$y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_k X_{ki} + \varepsilon_i $$
- 分类2:
	- 线性回归
		- 简单线性回归
		- 普通最小二乘回归
		- 多元线性回归
	- 非线性回归
		- 多项式回归
		- 其他
	- 逻辑回归
	

<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">相关性陷阱 (Correlation Pitfalls)</font>
- 在多元回归建模时，当心相关性陷阱
	
- 定义：只看到两个变量之间有相关关系，就误以为它们之间存在因果关系。
- 🌰：冰淇淋销量 和 溺水人数 正相关 (Correlation > 0.8)
	- 误以为 两者有因果关系
	- 事实是：**夏天气温↑** → **冰淇淋销量↑**，**夏天人们游泳多** → **溺水人数↑**
	- 它们共同受到了 气温 这个第三因素影响 (叫做混杂变更 confounder
- 影响：
	- 忽略了相关性陷阱 错误建模， 如：吃冰淇淋多了 容易溺水。
- 如何避免？
	- 进行随机对照
	- 控制混杂变量
	- 用因果推断技术 (比如DAG、因果推断模型)
	

<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">皮尔逊相关系数 (Pearson Correlation Coefficient)</font>

- 🎯 目标：衡量两个变量之间**线性相关程度**的统计量。
	- `pandas_data.corr()` 查看相关性系数
- 🧠： 两个变量 **X 和 y <font style="background-color:yellow; color:black">是否线性相关</font>、相关性多强**，以及是**正相关还是负相关**。
	
- 公式：$$r = \frac{\text{cov}(X, Y)}{\sigma_X \cdot \sigma_Y}  = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \cdot \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}} $$
	- $\bar{x}$：X 的均值
	- $\bar{y}$：Y 的均值
	- 分子是 **协方差**
	- 分母是 **标准差的乘积**
	

### 简单线性回归 (Simple Linear Regression)
[✍️SimpleLinearRegression-Jupyter Lab](./jupyter-notes/IBM-1-1-SimpleLinearRegression.ipynb)
- 🎯 目标：用于预测一个连续的变量
- ==最佳拟合 (best fit)==
	- 找到一条最能描述 **X** 和 **y** 之间关系的线（或平面），这个线就是我们要找的 **回归方程**。

### 普通最小二乘法回归（OLS）

<font style="background-color:yellow; color:black">Ordinary Least Squares Regression</font>

- OLS回归的特点：
	- 易于理解和解释
	- 不需要调参： 只需要提供数据
	- 回归解是只需要通过计算得到的
	- 准确值会受到 **异常值(outlier)** 的极大影响
	
- 为什么普通最小二乘法（OLS）回归在复杂数据集上的准确性有限？
	- 对于复杂数据集而言，OLS可能无法捕捉到变量之间的非线性关系或高阶交互作用，因此其预测能力会受到限制。

<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">最小二乘解</font>
**Least Squares Solution**（最小二乘解）
	
- 目标是：找到一组参数，使得模型预测值与实际数据的**误差平方和**最小。
- 特点：
	- 标准线性回归中，最优解就是最小二乘解（Least Squares Solution）。
	- *正则化模型（如岭回归、Lasso）或其他损失函数（如绝对误差）*，的最优解不是最小二乘解


### 多元线性回归
[✍️MultipleLinearRegression-Jupyter Lab](./jupyter-notes/IBM-1-2-MultipleLinearRegression.ipynb)
比简单线性回归相比，有<font style="background-color:yellow; color:black">多个输入变量</font>


### 多项式回归 （polynomial regression）

- 定义：用**多项式**来拟合数据点，从而捕捉非线性趋势
- 本质上还是线性回归
	- 多项式回归是非线性回归，但一般可以转化成线性回归。
	- ![[Pasted image 20250415163617.png]]
- 容易过度拟合![[Pasted image 20250418160750.png]]
- <font style="background-color:orange; color:black">多项式回归比线性回归更容易过拟合</font>（Overfitting） 


### 其他非线性回归 (Nonlinear Regression)
![[Pasted image 20250415163345.png]]
- 定义：用 <font style="background-color:orange; color:black">非线性方程式</font> 表示
	- 方程式可以是： 多项式，指数, 对数或非线性函数
- 目标：用于复杂关系回归
- 🌰 Examples:
![[nonlinear-curve-chinagdp.webp]]
	
	1. 中国GDP的指数级增长不能使用线性回归来分析
	2. 工作时长与生产力的对数增长回归曲线
	
- 其他非线性回归：
	- 回归树
	- 随机森林
	- 神经网络
	- 支持向量机 SVM
	- 梯度提升机 Gradient Boosting Machine
	- K近邻居 KNN
- 数据可视化 （如何知识我该采用哪种回归建模？）
	- **散点图**（scatter plot）
	- **变换线性化法（Transformation）**
		- 如果你怀疑是非线性关系，可以尝试“变换”数据让它线性化：
		- 如果怀疑是 **对数关系**：尝试 log(Y) 或 log(X) 再做线性回归    
		- 如果怀疑是 **指数关系**：尝试 log(Y) 对 X 做线性回归
		- 如果怀疑是 **幂函数关系**：尝试 log(Y) 对 log(X) 回归
	- **残差图分析（Residual Plot）📈**
		- 建模后，画出 **预测值 vs 残差图**
		- 如果残差呈现某种**系统性趋势**（如曲线、波动），说明线性模型不合适
		- 随机散布的残差 → 模型可能合适
    
	- **拟合优度指标（R²、AIC、BIC、MSE 等）📊**
		- 拟合不同模型后，比较这些指标：
		    - **R² 越接近 1 越好**
		    - **AIC/BIC 越低越好**
		    - **MSE 越小越好**

这些变换后，再用 **Pearson相关系数** 检查是否变得更线性。


### 逻辑回归 (Logistic Regression)

❌ 不是非线性回归方法，也不是回归模型，它是一个**线性模型**，预测**某个事件发生的概率**，用于解决**分类**问题
- 定义： **基于线性决策边界的非线性分类模型**
- 目标：在二维平面上，用直线把正例和反例分开，这就是决策边界(Decision Boundary)
- 公式：$$w_0 + w_1x_1 + w_2x_2 = 0$$
- 特点： 
	- 因为输出是通过sigmoid变成概率，所以整体是非线性映射
- 优点：
	- 训练速度快 
	- 概率输出
	- 不容易过拟合 (适合小数据集)
	- 特征解释性强
- 缺点：
	- 只能划线划边界，如果数据本身高度非线性，逻辑回归就力不从心了
	- 对异常值敏感
	- 要求特征独立
	- 容易欠拟合
- 什么时候用逻辑回归？
	- 二分类问题
	- 当需要计算输出结果的概率
	- 如果数据是线性可分的，逻辑回归的决策边界是一条直线，一个平面或超平面（n-1维结构）。
- 应用
	- 预测心脏病风险
	- 基于一系列特征诊断患者
	- 预测顾客是否会购买
	- 预测产品失败的可能性


<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">回归线和回归面</font>

![[Pasted image 20250418160344.png]]
回归线和回归面是**用数学模型对现实的一种近似抽象和拟合**，是一种“工具”，而不是客观存在的实体。
- **对数据的最佳线性拟合**，数据点本身并不都不在这条线/面上。


 <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">sigmoid 函数</font>
> <font style="background-color:yellow; color:black">把一个线性回归的输出，通过sigmoid函数转换成一个0到1之间的概率，然后根据这个概率来进行分类判断。</font>

- 数学表达式：
	- 假设线性回归的表达： $z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n$
	- 那么在逻辑回归中：$$\sigma(z) = \frac{1}{1 + e^{-z}}$$
- 这个函数就是 sigmoid函数![[Pasted image 20250415202645.png]]


<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">交叉熵损失函数（cross-entropy）</font>

 **🎯 用来衡量模型预测值和真实标签之间的差距，是逻辑回归的损失函数！**

| **名称**                                                             | **通常应用场景** | **是否等价**          |
| ------------------------------------------------------------------ | ---------- | ----------------- |
| <font style="background-color:tomato; color:black">Log-Loss</font> | 二分类        | ✅ 是交叉熵在二分类下的形式    |
| **Cross-Entropy**                                                  | 多分类更常见     | ✅ 广义形式（支持二分类和多分类） |

- 逻辑回归的损失函数是 **交叉熵损失函数**（cross-entropy）
- 表达式： $$\text{Loss} = -\sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$$




## 分类问题 (Classification)

![[Pasted image 20250428225214.png]]
- 定义：能够根据输入数据的特征，将其正确地归入预定义的、离散的==类别 (Class) ==或==标签 (Label) ==中
	- 分类问题 是[监督学习](./2.2%20机器学习的分类.md) 的一种核心问题。
- 应用
	- Email 过滤
	- Speech-To-Text
	- 书法识别
	- 生物识别
	- 分档分类
	- 客户服务
		- 客户留存预测
		- 客户分类
		- 广告响应
	- 贷款违约
	- 多分类药物处方
- 分类1:
	-  二分类 Binary Classification
	- 多重分类 Multi-Class Classification

<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">多重分类策略</font>

- 目标：把原本只能处理二分类问题的**分类器**，==**扩展**==到可以处理**多分类问题。**

假设你要分类「猫 🐱」、「狗 🐶」、「鸟 🐦」这三类：

 **🛝 OVR / OVA (One-Versus-Rest/All)**
- 如果有K个类型，就训练K个二分类器(Binary Classifier)
	- 训练k=3个分类器：
	    - 分类器1：<font style="background-color:yellow; color:black">猫 vs 非猫</font>（狗+鸟）
	    - 分类器2：狗 vs 非狗（猫+鸟）
	    - 分类器3：鸟 vs 非鸟（猫+狗）
- 预测：把样本分别输入到K个分类器中，选择概率最高的类别
- 优点：
	- 训练快
	- 计算效率高
	- 易于理解
- 缺点：
	- 类别不均衡问题时， 效果不佳
	- 分类器互相独立
	- 对噪音敏感

**🛝OVO (One-Versus-One)**
	
- 如果有K个类型，就每两个类型训练一个分类器，共训练$\frac{k*(k-1)}{2}$个分类器
	- 训练3个分类器（3 对 2 组合）：
	    - 分类器1：<font style="background-color:yellow; color:black">猫 vs 狗</font>
	    - 分类器2：<font style="background-color:yellow; color:black">猫 vs 鸟</font>
	    - 分类器3：狗 vs 鸟
- 预测：多数投票法。每个分类器投票，谁赢的次数最多就归为谁
- 优点：
	- 考虑类别间的关系，能够更精确地处理类别间的差异
	- 分类性能较好
- 缺点：
	- 计算量大
	- 复杂性高
	- 存储需求大


### 🥏 决策树 Decision Tree 
[✍️动手实验-决策树](../IBM-AI-Engineer-Course/jupyter-demo/IBM-1-3-1-DecisionTree.ipynb)

这里主要讨论决策树的 分类树分支， 回归树也是决策树的一种。

🌰 是否接Offer？
![[Pasted image 20250416192357.png]]

> 1. 决策树是通过**递归地把数据集分割成更小的部分**，来构建一棵“树”，最终用于对数据进行分类。
> 2. 在训练决策树时，每一步都要**选择一个“最能把数据分得清楚”的最好的特征**来做划分。

- 定义：本质是一棵<font style="background-color:yellow; color:black">树形图</font>，用来表示决策过程，树的叶子节点，代表一个分类
- 目标：用于数据分类 或 预测结果
- 优势：
	- 模型可视化
	- 可解释性强
	- 对特征工程要求低
		- 不需要特征缩放，如不需要fit_transform
		- 自动选择重要特征
	- 灵活的决策
- 分类

| **类型**                       | **用于任务**   | **输出结果**  | **举例**      |
| ---------------------------- | ---------- | --------- | ----------- |
| **分类树（Classification Tree）** | 分类任务（分类标签） | 类别（离散值）   | 是/否、红/绿/蓝 等 |
| **回归树（Regression Tree）**     | 回归任务（预测数值） | 连续的数值（实数） | 房价预测、体重预测等  |



<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">"Tree Pruning"（决策树剪枝）</font>
在决策树学习完成后，对其结构进行修改以减少其复杂性、提高其泛化能力并提升其可解释性的过程。

**为什么需要剪枝？**

1. **过拟合 (Overfitting):** 这是最主要的原因。未经过剪枝的决策树可能会过度学习训练数据的细节和噪声，导致决策边界过于复杂。
2. **提高泛化能力:** 简单的模型通常具有更好的泛化能力，能更好地处理新数据。
3. **提高效率和可解释性:** 较小的树在预测时需要的计算量更少，也更容易理解和解释。
4. **减少偏差:** 有时剪枝可以在一定程度上降低模型的偏差，使其不那么“天真”。


<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">停止条件（Stopping Criteria）</font>
不再继续分裂节点的条件
- 达到最小树高
- 当前节点样本数 小于 最小样本数
- 叶子节点数量达到最大数量
-  某个叶子节点中，**样本数少于你设置的最小值**


<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">选择最佳分裂点的标准</font>
如何测量最好的特征：

- 🍏 <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">信息增益 (Information Gain):</font>
	- 熵下降越多，信息增益越大，表示分裂后数据的不确定性减少得越多，分类结果越清晰
	- 公式：$$\text{Information Gain} = H(D) - \sum_{i=1}^{k} \frac{|D_i|}{|D|} H(D_i)$$
		- k, 有k个类别   
		- H(D)：当前数据集的熵
		- $D_i$：按某个特征划分后的子集
		- $H(D_i)$：子集的熵
		- $\frac{|D_i|}{|D|}$：子集所占的比例（加权）


- 🍏 <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">基尼不纯度 (Gini Impurity)</font>
	- 衡量从一个数据点随机地选取两个样本，其标签不一致的概率.
	- 基尼不纯度越低，表示数据集越纯净（即样本标签越一致）
	- 公式：$$\text{Gini} = 1 - \sum_{k=1}^{K} p_k^2$$
		- K 个类别 
		- $p_k$ 第 k 类在当前节点中的比例


#### 🥏 回归树 Regression Tree
[✍️回归树-DEMO](../IBM-AI-Engineer-Course/jupyter-demo/IBM-1-3-2-DecisionTree-RegressionTree.ipynb)
	
- 定义
	- 是决策树的一种
	- 当决策树适用于**解决回归问题**时， 这种决策树就是回归树

- <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">如何创建</font>
	
	- 递归分割数据集，直到达到 最大信息收益。
	- 减少分裂时数据类别的不确定性或随机性
	
- <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">数据分割规则</font>
	
	- 最小化真实值与预测值之间的差异
	- MSE 
	
- <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">怎么找 candidate threshold values？</font>
	- 特征排序后，相邻样本值的中点切割。



### 🥏 支持向量机 SVM (Support Vector Machine)
[✍️SVM-DEMO](../IBM-AI-Engineer-Course/jupyter-demo/IBM-1-3-3-SVM.ipynb)

![[Pasted image 20250417153141.png]]
[ Support Vector Machine(SVM)](https://sirfpadhai.in/support-vector-machinesvm/)

适合<font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); ">同类聚集</font>的数据集

- <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.3);">决策边界 (Decision Boundary)  / 分割平面(Separating hyperplane)</font>

	不同类别**分开**的线/面
	要分得**间隔最大**（也就是说，离这条线最近的点，尽可能远）

- <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.3);">支持向量 (Support Vector)</font>

	这些离超平面最近、起到决定作用的点，叫做**支持向量（Support Vectors）**。
	支持微量与决策边界之间的距离叫Margin
	
-  <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">决策边界的函数定义</font> 给定训练数据集
	- 公式：$$$(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$$
		- $x_i$ 是特征向量，
		- $y_i \in \{+1, -1\}$ 是类别，
- SVM就是要找到一个超平面，使得：$$w \cdot x + b = 0$$
		- w 是法向量，决定了超平面的方向
		- b 是偏置，决定了超平面的位置
	
- <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.3);">核技巧（Kernel Trick）</font>
	
	二维平面上的数据分布：🔵🟠🔵🟠🔵🟠，画直线无法给出决策边界
	解决办法：**升维**
![[Pasted image 20250417162951.png]]
	[Youtube - Kernel Trick](https://www.youtube.com/watch?v=N_RQj4OL1mg)
	
- 🍏 **优势**
	- 在高维空间更高效
	- 对过度拟合有强抵抗力
	- 在线性可分割数据上表现优秀
- 🍋‍🟩 **劣势**
	- 在大数据集上 训练过慢
	- 对噪音和重叠的分类敏感
- 🍌 **应用**
	- Image Classification
		- handwritten digit recognition
	- Parsing, Spam detection
	- Sentiment Analysis
	- Speech Recognition 
	- Anomaly Detection
	- Noise filtering
	
### 🥏 K-最近邻居 KNN (K-Nearest Neighbors)
[✍️KNN-DEMO](../IBM-AI-Engineer-Course/jupyter-demo/IBM-1-3-4-KNN.ipynb)

- 找出**离它最近的 K 个邻居**，看他们都是什么类别，
- **用邻居们的类别投票**，决定新样本属于哪一类！
![[KNN.webp]]

K是一个 **超参数（hyperparameter）**，需要我们**手动设定**或者**交叉验证（cross-validation）选出来**的

| 🔴 🔴 🔴<br>       🔵 🔵<br>       🔵 🔵 | 🔴🔵🔴🔵🔴<br><br>🔵🔴🔵🔴🔵 |
| ---------------------------------------- | ---------------------------- |
只适合<font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); ">低维+同类聚集</font>的数据集, 左边的数据集使用KNN效果较好


- <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">KNN的工作流程</font>：
	**KNN = 给点算距离 → 找最近邻居 → 多数投票 or 平均数 → 得出答案。**
	1. 设置一个K的值
	2. 针对每一个要预测的样本点
		- 计算它到所有已知标签的点的距离
		- 选出距离最近的K个点
		- 基于这K个邻居做预测
- <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">距离的计算</font>
	- 欧几里得距离 $d = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$
	- 曼哈顿距离 $D(A, B) = \sum_{i=1}^{n} |a_i - b_i|$
- <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">K值的影响</font>
- k小（比如k=1）→容易过拟合、对噪音敏感。
- k大（比如k=100）→可能过于平滑，失去细节。
- <font style="background-color: #FFFACD; color: #333333; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">如何确定K的值</font>
	- **经验法则** （常用起点）
		`K = sqrt(n)`
	- **交叉验证** cross-validation
		1. 划分训练集和验证集（或直接使用交叉验证）。
		2. 尝试一系列 $K$ 值（如 K=1,3,5,…,20K=1,3,5,…,20）。
		3. 计算每个 K 的平均准确率、F1分数等指标。
		4. 选择指标最优的 K，或在准确率与复杂度之间权衡。
	- **肘部法则（Elbow Method）**
		适用于关注模型稳定性的场景：
		- 绘制不同 KK 值的误差率（如分类错误率或均方误差）。
		- 选择误差下降趋缓的“拐点”对应的 KK。
		
- <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">懒惰学习器（lazy learner）</font>
	**KNN没有真正的训练过程**

### 🥏 集成模型 (Ensemble Models)

| **方法**           | **解释**                                                                                                                                                                       | **例子**              |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| **Bagging**（装袋）  | 多个模型<font style="background-color: #FFFACD; color: #333333; padding: 2px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1);">并行训练</font>，最后**投票/平均**。减少**方差**。   | 随机森林（Random Forest） |
| **Boosting**（提升） | 多个模型<font style="background-color: #FFFACD; color: #333333; padding: 2px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1);">串行训练</font>，每次**修正上次的错误**。减少**偏差**。 | XGBoost, LightGBM   |
#### 🥏 随机森林 (Random Forest)
> 构建多个决策树并将它们的结果进行综合
	
- 🧠 **如何工作的？**
	1. 采样出多个子数据集
	2. 训练多个决策树，每棵树随机选择部分特征，这样每棵树都有差异
	3. 结果汇总
		- 分类任务：多数投票决定
		- 回归任务：取所有预测结果的平均值
- 🌟优点：
	- 准确率高：更稳定不容易过拟合
	- 抗噪声强：对异常数据不第三
	- 自动处理缺失值和不平衡数据
- 缺点：
	- 模型大，训练和预测慢
	- 不如深度学习那样处理高维特征的能力强
	- 难以解释每个具体预测的过程，不如决策树的可视化清晰
	
#### 🥏 极端梯度提升 (XGBoost)
> 全称是 **eXtreme Gradient Boosting**. 属于Boosting家族，经常在kaggle中赢得冠军

[✍️RandomForest_XGBoost DEMO](../IBM-AI-Engineer-Course/jupyter-demo/IBM-1-3-5-RandomForest_XGBoost.ipynb)

让一个又一个的**小模型连续训练**，每个新模型努力**修正**前面的错误，最终把 小模型的能力**积累**起来，变成超强模型。

- 分类
	- XGBClassifier 分类器
	- XGBRegressor 回归器
- 对传统Boosting做大量工程优化：
	- 更快的训练速度
	- 更好正则化
	- 支持并行计算
	- 可处理缺失、支持自定义损失函数
	