
<h1 style=" text-align: center; font-size: 3em; font-family: 'Georgia', serif; color: #2c3e50; margin: 0.5em 0; padding: 10px 0; border-top: 4px solid #3498db; border-bottom: 4px solid #3498db; text-transform: uppercase; letter-spacing: 3px;"> 梯度下降与损失函数</h1>


### 损失函数 Loss Function
- **目的**：用于==衡量== 模型预测值 与 真实数据标签 之间的==**偏差**==
- **输入**： 是所有 **模型参数**
- **输出**：是一个实数
- 损失函数**值的大小** 反映 **模型在训练数据上的拟合程度**。
	- 仅反映误差，不能直接反映模型预测的性能
	- 损失函数值越小，模型预测可能越准确
	
### 梯度 (Gradient)
- 📖 定义：描述标量函数在某一点处的**最大变化率方向**以及该方向上的**变化率大小**。
	- 方向： 梯度向量指向函数在该点**上升最快的方向**
	- 大小：梯度的模长（即向量的长度）表示函数在该方向上的**最大变化率**。
- 特点：
	- 梯度是**一个多维向量**，是包含函数在该点上对==所有变量的偏导数==的集合。
- 梯度为零：
	- 当梯度为0, 意味函数在该点任意方向的变化率都为0
	- 这通常发生在临界点上，如局部最小/大值或**鞍点**
- 梯度计算与求偏导：
	- 求偏导：$f(x)$对x求偏导，会**假设**其他所有变量都是常数，得到的结果是一个数值
	- 梯度计算：计算一个函数$f(x)$对其===所有变量的偏导数=== ，得到的结果是向量
	- 计算函数对某个变量的梯度：
		- 指函数在该变量上的偏导数
		- 也是梯度向量中的一个分量
	
### 📉 梯度下降 Gradient Descent
	
- 📖 定义：
	>不断==调整模型参数==，来最小化损失函数。
- 🎯 目标：==努力寻找 Loss 函数的全局最小值==
- 🧠 基本思想：
	- 沿着当前点的梯度方向前进一小步，就能让目标函数的值下降。
	- 沿着目标函数==下降最快的方向==移动，找到损失函数最小值
- 公式：$$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta_t)$$
	-  $\theta$：模型的参数（例如，神经网络中的权重和偏置）。
	- $\eta$：学习率（Learning Rate），控制步长的大小。
	- $J(\theta)$: 损失函数
	- $\nabla_\theta J(\theta_t)$：损失函数 $J(\theta)$ 对参数 $\theta$ 的梯度，表示损失函数相对于每个参数的变化率。
	
- 梯度下降和反向传播的一些本质区别：
	- 梯度下降负责更新参数，而反向传播只计算梯度，不负责更新参数
	- 梯度下降一般没有层，不需要记录中间状态；反向传播需要记录每一层的输出
	
- 梯度下降的目标就找Loss函数的最小值
![[Pasted image 20250430151152.png]]
- 假设 Loss 函数： $y=f(w_1)$
- 目标 🎯：理想情况下，梯度下降的目标是找到损失函数的，==全局最小值(global minimum),== 如左图 $w_{min}$ 
	- 求 loss 函数极小值 (local minima) 就是==找微分 (也就导数) 是 0 的 w 值==.
- 挑战:
	- 梯度下降不一定总是能够找到全局最小值
	- 往往只能找到一个局部最小值(local minima), 或某个较低的优化点 (saddle points), 如右图 $w_{min}$
	
- **📉 寻找 局部极小值 和 全局极小值**
![[GradientDescentGIF.gif]]
- 工作流程：
	- 计算点所在斜率
	- 斜率为正往左走，斜率为负往右走
	- 斜率较大多走两步，快速下降；斜率较小， 少走两步；
	- 最终找到 global minima
#### - 多元函数的梯度下降
多元函数的梯度是一个向量，表示多元函数在一个点上 ==变化最快的方向和变化率==
![[multi-gradient-descent.gif]]

#### 收敛
- 梯度下降通过多次迭代逐渐逼近最小值。
- 挑战 - 局部最优：
	- 如果函数是**凸函数**(只有一个最低点)，梯度下降会收敛到全局最优解
	- 如果函数是**非凸函数**，可能陷入局部最优。
- 如何缓解局部最小值问题？
	- 调整学习率
	- 改进初始化方法
	- 使用合适的优化器

#### 万能逼近定律（Universal Approximation Theorem）
- 定义：
	 > **一个==足够深或足够宽==的神经网络可以在理论上逼近任意连续函数（在限定区间内）到任意精度**。
- 🌰 例子：
	- 单层前馈神经网络（单隐层）在激活函数是非线性（如 Sigmoid）的情况下可以逼近任意连续函数
	- 前提是神经元数量足够大。
- 意义：
	- 理论保证了神经网络的强大能力
		- 只能网络结构足够大，并且有适当的非线性激活函数，神经网络就能学习到几乎任何形式的办入输出映射
	- 设计神经网络的灵活性：设计时不需要太过担心网络是否能表达所需的复杂关系。
	- 解释和信息：
		- 根据UAT，神经网络有理论上的能力去逼近任何复杂的函数，梯度下降真正实现了这个理论。
- 局限性：
	- 需要大量神经元
	- 函数的光滑性要求： 只适用于连续函数
	- 非实际的理想性情：定律只证明了可能性，在实际中，受训练数据、算法、网络结构、正则化等多方面影响

#### 梯度下降的种类
	
+ **批量梯度下降（Batch Gradient Descent）**
	- 每次使用所有的训练数据来计算梯度并更新参数。
+ **随机梯度下降（Stochastic Gradient Descent, SGD）**
	- 每次使用一个样本来计算梯度并更新参数。
+ **小批量梯度下降（Mini-batch Gradient Descent）**
	- 每次使用一小部分（mini-batch）数据来计算梯度并更新参数。
	
#### 梯度消失 (Vanishing Gradient)
- 📖 定义：
	> 梯度消失是指在反向传播过程中，**梯度的值逐渐变小，趋近于零**，
	> 导致模型参数的更新幅度趋近于零，从而无法学习到有效的特征表示。
- 数学背景：
	- 梯度是通过逐层反向传播计算的
	- 每一层的梯度是==前一层梯度与当前层权重矩阵的乘积==
	- 当网络层数较多时，如果每一层的梯度都小于 1，乘积会迅速趋近于零，导致**梯度消失**
- 💥 影响：
	- 网络**深层参数无法更新**，导致模型无法有效学习
		- 常见于使用Sigmoid 或 Tanh等饱和激活函数

#### 梯度爆炸（Exploding Gradient）
- 定义：
	> 梯度爆炸是指在反向传播过程中，**梯度的值变得非常大**（趋于无穷）
	> 导致参数更新幅度过大，模型无法收敛，甚至数值溢出（NaN）。
- 数学背景：
	- 某一层的梯度（或权重值）大于 1，多次乘积会导致梯度指数级增长。
	- 权重矩阵 WWW 的“条件数”过大，这点会更加明显
	- 当网络层数较多时，如果每一层的梯度都大于 1，乘积会变得非常大，导致**梯度爆炸**
- 影响：
	- 参数更新幅度过大，**模型发散**；
	- 数值计算不稳定（如出现无穷大或 NaN）。
#### 缓解梯度问题 （Mitigating Gradient Issues）

+ **权重初始化**
	+ 使用合理初始化方法（如 He Initialization 或 Xavier Initialization）；
    - 避免权重初始化过大或过小。
+ **使用合适的激活函数**
	- 避免使用 Sigmoid/Tanh 等饱和激活函数；
	- 改用 ReLU 系列（如 Leaky ReLU、Parametric ReLU）或其他非线性激活函数。
+ **梯度裁剪（Gradient Clipping）**
	+ 限制梯度的值在合理范围，防止爆炸。
+ **Batch Normalization**
	+ 通过归一化激活值，使梯度更稳定。
- **残差连接**（Residual Connections）：
	- 在深层网络中引入跳跃连接，缓解梯度消失问题（如 ResNet）
+ **使用适应性优化算法**
	+ 使用自适应优化器（如 Adam、RMSProp）替代传统的 SGD。
- **网络结构调整**：
    - 对于 RNN，改用 LSTM（Long Short-Term Memory）或 GRU（Gated Recurrent Unit）结构，避免梯度爆炸/消失。
	

## 自动求导 (autograd)

- 传统机器学习通常不需要自动求导
	- 因为像传统ML模型如SVN，其导数可以直接用公式写出而不需要自动求导工具。

回归 深度学习的核心思想：

模型训练的本质 = 不断更新参数（<font style="background-color:#FBDE28;color:black">权重和偏置</font>），使 <font style="background-color:#FBDE28;color:black">损失函数最小化</font>

—— 所以：我们要找<font style="background-color:#FBDE28;color:black">最优解（最小值）</font>

—— 找最小值：要用<font style="background-color:#FBDE28;color:black">梯度下降</font>

—— 梯度下降：要先求导

—— 求导过程：由 <font style="background-color:#FBDE28;color:black">自动微分/自动求导机制</font> 完成


自动微分是一种计算导数的技术，它通过记录**前向传播**过程中每个操作的<font style="background-color:#FBDE28;color:black">中间结果和操作本身</font>，然后反向逐步计算各个变量对目标函数的导数（<font style="background-color:#FBDE28;color:black">梯度</font>）。

<font style="background-color:#FBDE28;color:black">反向传播是实现自动求导的一种算法</font>（在神经网络中）。

#### 代码示例
```python
import torch

# 创建一个需要梯度的张量
x = torch.tensor(2.0, requires_grad=True)
y = x ** 2  # y = x^2

# 计算梯度
y.backward()  # dy/dx = 2x
print(x.grad)  # 输出: tensor(4.) (因为 x=2, 2*2=4)
```

以下打开 github 代码查看：

+ 动态计算图
+ 梯度积累和清零
+ 阻止梯度跟踪
+ 高阶梯度


## 参考资料

[Neural Networks and Deep Learning（四）图解神经网络为什么能拟合任意函数 | bitJoy](https://bitjoy.net/2019/04/07/neural-networks-and-deep-learning%ef%bc%88%e5%9b%9b%ef%bc%89%e5%9b%be%e8%a7%a3%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ba%e4%bb%80%e4%b9%88%e8%83%bd%e6%8b%9f%e5%90%88%e4%bb%bb%e6%84%8f%e5%87%bd/)
