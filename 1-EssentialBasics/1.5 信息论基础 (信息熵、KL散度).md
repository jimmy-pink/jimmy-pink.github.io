```text
信息论基础
├── 1. 信息量（Self-Information）
│   ├── 直觉: 事件越少见，信息量越大

├── 2. 熵（Entropy）
│   ├── 定义: H(X) = -Σ P(x) log₂ P(x)
│   ├── 含义: 随机变量 X 的平均信息量

├── 3. 联合熵（Joint Entropy）
│   ├── 含义: 联合随机变量(X,Y)的信息量

├── 4. 条件熵（Conditional Entropy）
│   ├── 定义: H(Y|X) = -ΣΣ P(x,y) log₂ P(y|x)
│   ├── 含义: 已知X后，Y还剩下多少不确定性

├── 5. 互信息（Mutual Information）
│   ├── 定义: I(X;Y) = H(X) - H(X|Y)
│   ├── 等价形式: I(X;Y) = ΣΣ P(x,y) log₂ (P(x,y) / (P(x)P(y)))
│   ├── 含义: X 和 Y 共享的信息量
│   ├── 特性: 
│   │   ├── 非负性: I(X;Y) ≥ 0
│   │   ├── 对称性: I(X;Y) = I(Y;X)

├── 6. 相对熵 / KL散度（Kullback–Leibler Divergence）
│   ├── 定义: D_KL(P || Q) = Σ P(x) log₂ (P(x)/Q(x))
│   ├── 含义: P 和 Q 分布之间的“距离”度量（不是对称的）
│   ├── 常见误区: KL散度不是数学意义上的“距离”，不满足对称性

├── 7. 香农第一定理（无失真压缩极限）
│   ├── 内容: 任意数据压缩的极限在熵 H(X) 左右
│   ├── 含义: 平均编码长度 ≥ 熵

├── 8. 香农第二定理（信道编码定理）
│   ├── 内容: 若信息速率 < 信道容量，则可以用极小错误率可靠通信
│   ├── 信道容量: C = max_{p(x)} I(X;Y)

├── 9. 其他重要概念
│   ├── 冗余（Redundancy）
│   │   ├── 定义: 实际编码长度 - 熵
│   │   ├── 含义: 描述编码效率
│   ├── 熵率（Entropy Rate）
│   │   ├── 定义: 长度趋向无穷时，每个符号的平均熵
│   │   ├── 用于：时间序列，马尔可夫过程
```


### 1. 信息量

- 定义：事件 x 发生时携带的信息量，记作 I(x)
- 🧠 概率越小，事件越“稀有”，信息量越大。
- 公式：$$I(x) = -\log_b p(x)$$
	-  $p(x)$：事件 x 发生的概率。
	- b：对数的底（通常是2，叫**以2为底**，单位是**比特**）。
	
### 2. 熵 (Entropy)

 ==A way of measuring the **lack of order** that exists in a system== 测试系统 **混乱** 程度
 
熵越高，混乱程度越大。熵下降越多，信息增益 (Information Gain) 就越大
	
- 定义：一个随机变量所有可能事件的信息量的加权平均。
- 🧠 **平均每次获取的信息量**
- 公式：$$H(X) = -\sum_{i} p(x_i) \log_b p(x_i)$$
	- H(X)：随机变量X的熵
	- $x_i$：X可能取的第i个值。
	
### 3. 联合熵 (Joint Entropy)
	
- 定义：随机变量 X 和 Y 共同发生时的信息量
- 🧠 X和Y一起发生的不确定性有多少？
- 公式：$H(X,Y) = -\sum_{x,y} p(x,y) \log_b p(x,y)$
	
### 4.条件熵 (Conditional Entropy)
	
- 定义：在已知Y的情况下，X仍然带来的不确定性
- 🧠 Y知识后，X的未知程序
- 公式：$H(X|Y) = \sum_{y} p(y) H(X|Y=y)$

### **5. 互信息（Mutual Information）**

- 定义：X 和 Y 之间互相带来的信息量。
- 公式: $I(X;Y) = H(X) - H(X|Y)$

### **6. 相对熵（KL散度，Kullback-Leibler Divergence）**
	
- 定义：两个分布 P 和 Q 的距离（实际上不是严格的距离）。
    
- 公式：$D_{\text{KL}}(P \| Q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)}$
  