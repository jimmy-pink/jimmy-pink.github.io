

## **ä¸€ã€ç†è§£ Keras çš„çµæ´»æ€§è¾¹ç•Œ**

Keras æä¾›ä¸‰ç§ä¸»è¦çš„å¼€å‘æ¨¡å¼ï¼š

- **å‡½æ•°å¼ APIï¼ˆFunctional APIï¼‰**ï¼šç”¨äºå¿«é€Ÿæ­å»ºæ ‡å‡†ç¥ç»ç½‘ç»œç»“æ„ã€‚
    
- **å­ç±»åŒ–æ¨¡å‹ï¼ˆModel Subclassingï¼‰**ï¼šå…è®¸ä½ è‡ªç”±å®šä¹‰æ¨¡å‹ç»“æ„å’Œå‰å‘ä¼ æ’­é€»è¾‘ã€‚
    
- **è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼ˆCustom Training Loopsï¼‰**ï¼šä½ å¯ä»¥å®Œå…¨æ§åˆ¶å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€ä¼˜åŒ–å’Œæ—¥å¿—è®°å½•ç­‰æµç¨‹ã€‚
    

é«˜é˜¶ç”¨æ³•ä¸»è¦å›´ç»•å­ç±»åŒ–ä¸è‡ªå®šä¹‰è®­ç»ƒå±•å¼€ã€‚

---

## **äºŒã€è‡ªå®šä¹‰è®­ç»ƒæµç¨‹ï¼šæŒæ§æ¯ä¸€æ­¥**

  

å½“ä½ éœ€è¦æ›´ç²¾ç»†åœ°æ§åˆ¶è®­ç»ƒè¿‡ç¨‹ï¼ˆå¦‚ä½¿ç”¨éæ ‡å‡† lossï¼ŒåŠ å…¥ç‰¹å®šç›‘æ§é€»è¾‘ç­‰ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ GradientTape æ‰‹åŠ¨ç¼–å†™è®­ç»ƒå¾ªç¯ï¼Œæˆ–åœ¨ Model å­ç±»ä¸­é‡å†™ train_step æ–¹æ³•ã€‚


**ä¼˜ç‚¹**ï¼š
- è‡ªå®šä¹‰Losså‡½æ•°å’Œä¼˜åŒ–æ–¹æ³•
- é«˜çº§æ—¥å¿—å’Œç›‘æ§
-  ç ”ç©¶ä¸å¼€å‘çš„çµæ´»æ€§
- é›†æˆè‡ªå®šä¹‰æ“ä½œå’Œæ¨¡å‹å±‚ 

**å¸¸è§åœºæ™¯åŒ…æ‹¬ï¼š**

- å¤šä»»åŠ¡å­¦ä¹ ï¼šåŒæ—¶è®­ç»ƒå¤šä¸ªè¾“å‡ºå’Œ lossã€‚
    
- åŠ¨æ€ loss åŠ æƒï¼šæ ¹æ® epoch è°ƒæ•´æŸå¤±å‡½æ•°æ¯”é‡ã€‚
    
- å¼ºåŒ–å­¦ä¹ æˆ– GANï¼šéœ€å¯¹è®­ç»ƒæ­¥éª¤è¿›è¡Œå®šåˆ¶æ§åˆ¶ã€‚
    

- ğŸŒ° ä½¿ç”¨GradientTapeçš„è‡ªå®šä¹‰è®­ç»ƒ
```python
import tensorflow as tf

model = MyModel()
optimizer = tf.keras.optimizers.Adam()
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()

for epoch in range(epochs):
    for x_batch, y_batch in dataset:
        with tf.GradientTape() as tape: #è®°å½•å‰å‘ä¼ æ’­ä¸­çš„è¿ç®—
            logits = model(x_batch, training=True)
            loss = loss_fn(y_batch, logits)
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

-  ğŸŒ° è‡ªå®šä¹‰è®­ç»ƒæ–¹æ³•ï¼šé‡å†™Modelçš„train_step
é€‚ç”¨äºmodel.fit()æ¡†æ¶ä¸‹éœ€è¦æ›´çµæ´»é€»è¾‘çš„æƒ…å†µï¼š
```python
class MyModel(tf.keras.Model):
    def train_step(self, data):
        x, y = data
        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)
            loss = self.compiled_loss(y, y_pred)
        gradients = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}
```


---

## **ä¸‰ã€è®¾è®¡è‡ªå®šä¹‰ Layer ä¸æ¨¡å‹ç»„ä»¶**

  

å¯¹äºå¤æ‚ç½‘ç»œç»“æ„ã€ç¨€æœ‰æ“ä½œæˆ–ä»£ç å¤ç”¨éœ€æ±‚ï¼ŒKeras æ”¯æŒé€šè¿‡ç»§æ‰¿ Layer æˆ– Model ç±»åˆ›å»ºè‡ªå®šä¹‰æ¨¡å—ã€‚

- ğŸŒ° è‡ªå®šä¹‰Layer
```python
class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal')
        self.b = self.add_weight(shape=(self.units,), initializer='zeros')

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
```
- ğŸŒ°è‡ªå®šä¹‰æ¨¡å‹ç»“æ„ (å­ç±»åŒ–Model)
```python
class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(64, activation='relu')
        self.dense2 = tf.keras.layers.Dense(10)

    def call(self, inputs):
        x = self.dense1(inputs)
        return self.dense2(x)
```


### **3.1 è‡ªå®šä¹‰ Layer çš„å…¸å‹ç”¨é€”**

- å®ç°æ–°é¢–çš„å‰å‘ä¼ æ’­é€»è¾‘ï¼ˆå¦‚ capsuleã€attention ç­‰ï¼‰
    
- ç®¡ç†æƒé‡çš„åˆå§‹åŒ–ä¸ä¿å­˜
    
- æ·»åŠ è‡ªå®šä¹‰ loss æˆ– metric
    

  

### **3.2 è‡ªå®šä¹‰ Model çš„å¸¸è§åœºæ™¯**

- å¤šåˆ†æ”¯æ¨¡å‹ï¼ˆå¦‚ Siamese ç½‘ç»œï¼‰
    
- è¾“å‡ºç»“æ„å¤æ‚ã€æ— æ³•ç”¨ Sequential æˆ– Functional è¡¨è¾¾
    
- å†…åµŒå¤šä¸ªè‡ªå®šä¹‰å­æ¨¡å‹

---

## **å››ã€é«˜çº§å›è°ƒå‡½æ•°ï¼šè®­ç»ƒè¿‡ç¨‹çš„æ§åˆ¶ä¸­å¿ƒ**

  

Keras çš„ Callback ç³»ç»Ÿæä¾›è®­ç»ƒä¸­é€”å¹²é¢„ã€æ—¥å¿—è®°å½•ã€ä¿å­˜æ¨¡å‹ã€åŠ¨æ€è°ƒæ•´è¶…å‚æ•°ç­‰åŠŸèƒ½ã€‚
  

**å¸¸ç”¨å›è°ƒåŒ…æ‹¬ï¼š**

| **å›è°ƒå**               | **åŠŸèƒ½æè¿°**      |
| --------------------- | ------------- |
| ModelCheckpoint       | ä¿å­˜æœ€ä¼˜æ¨¡å‹æˆ–å‘¨æœŸæ€§æ£€æŸ¥ç‚¹ |
| EarlyStopping         | è‡ªåŠ¨åœæ­¢è®­ç»ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ  |
| ReduceLROnPlateau     | ç›‘æ§æŒ‡æ ‡æ— æå‡æ—¶é™ä½å­¦ä¹ ç‡ |
| TensorBoard           | è®°å½•è®­ç»ƒè¿‡ç¨‹ï¼Œæ”¯æŒå¯è§†åŒ–  |
| LearningRateScheduler | åŠ¨æ€è°ƒèŠ‚å­¦ä¹ ç‡ç­–ç•¥     |
é«˜çº§ç”¨æˆ·è¿˜å¯ä»¥é€šè¿‡ç»§æ‰¿ Callback ç¼–å†™è‡ªå®šä¹‰å›è°ƒï¼Œç”¨äºæ—¥å¿—ä¸Šä¼ ã€æ¨¡å‹ç²¾åº¦è¯„ä¼°ç­‰åŠŸèƒ½ã€‚

- ğŸŒ° è‡ªå®šä¹‰Callback
```python
class CustomCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs['val_accuracy'] > 0.9:
            print("Validation accuracy over 90%, stopping training.")
            self.model.stop_training = True
```

---

## **äº”ã€æ··åˆç²¾åº¦ä¸åˆ†å¸ƒå¼è®­ç»ƒ**

  

Keras æ”¯æŒé€šè¿‡ä»¥ä¸‹æ‰‹æ®µæå‡è®­ç»ƒæ•ˆç‡ï¼Œå°¤å…¶é€‚ç”¨äºå¤§å‹æ¨¡å‹æˆ–å¤š GPU ç¯å¢ƒï¼š

  

### **5.1 æ··åˆç²¾åº¦è®­ç»ƒï¼ˆMixed Precisionï¼‰**

  

å¯ç”¨åï¼Œæ¨¡å‹å°†è‡ªåŠ¨åœ¨ä¿æŒæ•°å€¼ç¨³å®šæ€§çš„å‰æä¸‹ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ï¼ˆfloat16ï¼‰è¿›è¡ŒåŠ é€Ÿï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚

- ğŸŒ° æ··åˆç²¾åº¦è®­ç»ƒ
```python
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')
```

### **5.2 åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥**

  - ğŸŒ° å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
```python
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model = create_model()
    model.compile(optimizer, loss, metrics)
```

TensorFlow çš„åˆ†å¸ƒå¼ç­–ç•¥ï¼ˆå¦‚ MirroredStrategyï¼‰å¯ç”¨äºå¤šå¡åŒæ­¥è®­ç»ƒï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®æˆ–æ¨¡å‹ã€‚

---

## **å…­ã€æ„å»ºè‡ªå®šä¹‰ Loss ä¸ Metric**

  

åœ¨ä¸€äº›ç‰¹å®šä»»åŠ¡ä¸­ï¼ŒKeras é»˜è®¤çš„æŸå¤±å‡½æ•°å’ŒæŒ‡æ ‡å¯èƒ½æ— æ³•æ»¡è¶³éœ€æ±‚ï¼Œä¾‹å¦‚ï¼š

- å¼‚æ„ä»»åŠ¡çš„è‡ªå®šä¹‰æŸå¤±ç»„åˆ
    
- å¤šæ ‡ç­¾åˆ†ç±»ä¸‹çš„è‡ªå®šä¹‰ F1-score
    
- ç‰¹å®šä¸šåŠ¡éœ€æ±‚ä¸‹çš„åŠ æƒè¯¯å·®æƒ©ç½š

ä½ å¯ä»¥ç»§æ‰¿ Loss æˆ– Metric ç±»ï¼Œå°è£…é€»è¾‘å¹¶ä¸ model.compile() é…åˆä½¿ç”¨ï¼Œäº«å—æ ‡å‡†è®­ç»ƒæ¡†æ¶å¸¦æ¥çš„æ˜“ç”¨æ€§ã€‚

- ğŸŒ° è‡ªå®šä¹‰Losså’ŒMetric
```python
class MyCustomLoss(tf.keras.losses.Loss):
    def call(self, y_true, y_pred):
        return tf.reduce_mean(tf.square(y_true - y_pred))

class MyMetric(tf.keras.metrics.Metric):
    def __init__(self, name="my_metric", **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name="total", initializer="zeros")

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.total.assign_add(tf.reduce_sum(tf.abs(y_true - y_pred)))
```

---

## **ä¸ƒã€è¿›é˜¶æŠ€å·§è¡¥å……**

- add_metric() å’Œ add_loss()ï¼šå¯åœ¨ Layer ä¸­æ³¨å…¥è®­ç»ƒæŒ‡æ ‡å’Œé¢å¤–æŸå¤±ã€‚
    
- æ¨¡å‹å¾®è°ƒï¼ˆFine-tuningï¼‰ï¼šå†»ç»“éƒ¨åˆ†å±‚ï¼Œç»„åˆè®­ç»ƒç­–ç•¥ã€‚
    
- æ¨¡å—å¤ç”¨ï¼šå°†å¤šä¸ªè‡ªå®šä¹‰ Layer æ‰“åŒ…ä¸ºå­æ¨¡å—ï¼Œä¾¿äºåœ¨ä¸åŒé¡¹ç›®ä¸­è¿ç§»ã€‚
    


- ğŸŒ° add_metric, add_loss è‡ªå®šä¹‰æ¨¡å‹ä¸­åµŒå…¥æŒ‡æ ‡ä¸æŸå¤±
```python 
def call(self, inputs):
    output = self.dense(inputs)
    self.add_metric(tf.reduce_mean(output), name='output_mean')
    self.add_loss(tf.reduce_sum(output))
    return output
```


---