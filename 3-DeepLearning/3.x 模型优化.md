
## 数据层面优化

- 数据质量
	- 异常值处理
		- IQR
		- Z-score
- 特征工程
	- 特征选择
		- 方差阈值
		- 卡方检验
		- RFE
	- 特征变换
		- 标准化
		- 归一化
		- 对数变换
	- 特征构造
		- 交叉特征
		- 多项式特征
- 数据平衡
	- 过采样
	- 欠采样
	- 类别权重调

### 数据增强


---

## 模型调优
> 直接调整模型的结构、超参数、损失函数等，以提升性能。

### 模型选择
- 问题匹配
	- 线性问题
	- 非线性问题


### 超参数调优
- Grid Search
- Random Search
- Hyperopt

#### 学习率策略
- 学习率衰减 (Step/Exponential)
- 自适应优化器 (Adam/RMSprop)


### 模型复杂度权衡


#### 模型复杂度指数
| **模型类型**    | **主要复杂度指标**              |
| ----------- | ------------------------ |
| 线性回归        | 参数数量（特征维度+1）             |
| 深度神经网络（DNN） | 参数量、FLOPs、层数、宽度          |
| 卷积神经网络（CNN） | 参数量（卷积核）、感受野、FLOPs       |
| Transformer | 参数量（$d_{model}$、头数）、序列长度 |

#### 泛化能力评估
- 偏差-方差权衡


#### 模型参数评估
<font style="background-color:yellow; color:black">高信噪比，稀疏系数 的数据更优质</font>
**Sparse Coefficients（稀疏系数）**
大部分系数为0或接近0

#### 模型数据质量
**信噪比 SNR (Signal-to-Noise Ratio)**
信息量与噪音的比率



---
### 模型复杂度控制

#### 正则化技术 Regularization
	
- 定义：正则化是一个**防止过度**拟合的**回归技术**
	- 作用于 特征选择 阶段
	- 调整 **强噪音对应的特征** 的权重，从而在本轮训练不使用该特征。
- 基本思想：在训练时**限制模型**，防止模型的参数变得过大或过于复杂
	- 在损失函数中加入一个额外的项，用来**惩罚过于复杂的模型**。（去除无关或噪音特征）
	- 在普通线性回归中，目标是最小化loss函数
	
<img src="../images/L1-L2.webp" alt="L1-L2.webp">
![regularization-comparation.png](../images/regularization-comparation.png)

**L1正则化（Lasso Regularization）**
- 对模型参数的绝对值求和来增加惩罚项。
- 特点：
	- 特别适合处理稀疏数据
**L2正则化（Ridge Regularization）**
- 对模型参数的平方和进行惩罚。
**弹性网正则化（Elastic Net Regularization）**
- 也叫**L1/L2混合正则化**
- 弹性网正则化结合了L1正则和L2正则化的优点。
![regularization-plot-comparation.png](../images/regularization-plot-comparation.png)


#### Dropout
- 定义：一种用于DL模型训练中的**正则化技术**。
- 目的：
	- 减少过拟合，提高模型泛化能力
	- 减少神经元之间的共适应性
- 原理：
	- 在每次训练迭代中，随机丢弃一部分神经元，不参与当前计算。
		- 丢弃比例由超参数 dropout rate决定，通常在0.2-0.5之间
	- 使得模型训练过程中不会过度依赖某些神经元，从而减少过拟合。
- 缺点：
	- 增加训练时间
		- 有效计算量增加
	- 可能影响模型学习效率
		- 过度使用阻碍模型学习到足够的特征

#### **早停（Early Stopping）**
[2.x 模型评估与调优#早停 Early Stopping](2.x%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E8%B0%83%E4%BC%98#早停 Early Stopping)
- 早停也是一种正则化技术。
- 原理：
	- 在验证误差开始增大时提前停止训练
	- 避免模型训练得过长，防止开始记住训练数据中的噪声

---
## 高级优化技术
- 自动化机器学习 (AutoML)
- 神经架构搜索 (NAS)
- 迁移学习
	- 预训练模型微调
- 半监督学习
- 对抗训练

### 迁移学习
- 定义 🧠
	- 从一个任务中学到的知识（通常是通过预训练的模型）应用到与之相关的另一个任务中
	- 从而加速新任务的学习，并提高模型的性能。
- 核心理念
	- 利用已有的知识，特别是对大规模数据集上训练的DL模型，将其应用到目标任务上。
- 关键要点：
	- 已有知识
	- 相关性
		- 源任务和目标任务之间通常有一定的相关性
	- 提高效率和性能
	- **预训练模型**
		- 预训练模型是先在大规模数据集上训练得到的模型，然后将其在目标任务上进行微调(fine-tuning)
		- 迁移学习常常依赖于预训练模型
- 主要类型
	- 同源迁移 ： 源任务和目标任务属于同一个领域，但使用的数据分布可能略有不同
	- 异源迁移：不同领域。这是迁移学习最强大的形式
	- 多任务学习：同时学习多个相关任务。模型在多个任务上共享表示。
	- 领域适用：专注于处理不同数据分布之间的差异。
		- 目标：在同一领域上训练的模型能够很地应用于另一个领域
	
---
## 资源优化
- 计算加速
	- GPU/TPU利用
	- 混合精度训练
- 模型压缩
	- 量化 (8-bit/4-bit)
	- 知识蒸馏
	- 剪枝


