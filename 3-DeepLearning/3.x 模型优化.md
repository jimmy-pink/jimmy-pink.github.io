
## 数据层面优化

- 数据质量
	- 异常值处理
		- IQR
		- Z-score
- 特征工程
	- 特征选择
		- 方差阈值
		- 卡方检验
		- RFE
	- 特征变换
		- 标准化
		- 归一化
		- 对数变换
	- 特征构造
		- 交叉特征
		- 多项式特征
- 数据平衡
	- 过采样
	- 欠采样
	- 类别权重调

### 数据增强 Data Augmentation
- 定义
	- 从现有数据集中生成更多变异的训练样本
- 目标
	- 增强模型泛化能力
	- 缓解过拟合现象
	- 模拟数据分布的多样性
- 常见方法
	- 图像增强
		- 几何变换：旋转/ 平移/ 翻转 /缩放 zoom
		- 色彩扰动：亮度 / 对比度 / 饱和度 
		- 随机裁剪
		- 添加噪声 (Gaussian/Shot Noise)
		- 高级技术： 
			- Mixup/CutMix、风格迁移
			- featurewise_center **中心化** 减去整个数据集的平均值
	- 文本增强
		- 语义替换： 同义词/反义词替换
		- 语法变换： 句型重排/主动被动转换
		- 自动翻译
		- Concatenate 合成长文本
	- 音频增强
		- 时序变换：速度时间轴变换
		- 噪声流入：环境噪声叠加
		- 频谱扰动：频域调整
	- 通用方法
		- SMOTE 平衡分类数据
		- EDA 文本增强算法
		- 生成模型： GAN/VQ-VAE
- 变换策略
	- 保持基础数据本征特征
	- 保持正类/负类 样本方向
	- 动态调整变换强度
- 均衡控制
	- 类别平衡增强 防止过采样偏差
	- 真实分布逼近
- 效果评估
	- 观测指标
		- Test Accuracy提升
		- Faster Convergence 速度
		- Robustness到对抗攻击
	- 验证方法：
		- T-SNE特征可视化
		- 梯度敏感性分析
		- 00D检测能力测试
- 应用场景
	- 图像分类/目标检测/语义分割
	- NLP文本预训练
	- 医疗影像分析
	- 工业瑕疵检测



---

## 模型调优
> 直接调整模型的结构、超参数、损失函数等，以提升性能。

### 模型选择
- 问题匹配
	- 线性问题
	- 非线性问题


### 超参数调优
- Grid Search
- Random Search
- Hyperopt

#### 学习率策略
- 学习率衰减 (Step/Exponential)
- 自适应优化器 (Adam/RMSprop)


### 模型复杂度权衡


#### 模型复杂度指数
| **模型类型**    | **主要复杂度指标**              |
| ----------- | ------------------------ |
| 线性回归        | 参数数量（特征维度+1）             |
| 深度神经网络（DNN） | 参数量、FLOPs、层数、宽度          |
| 卷积神经网络（CNN） | 参数量（卷积核）、感受野、FLOPs       |
| Transformer | 参数量（$d_{model}$、头数）、序列长度 |

#### 泛化能力评估
- 偏差-方差权衡


#### 模型参数评估
==高信噪比，稀疏系数 的数据更优质==
**Sparse Coefficients（稀疏系数）**
大部分系数为0或接近0

#### 模型数据质量
**信噪比 SNR (Signal-to-Noise Ratio)**
信息量与噪音的比率



---
### 模型复杂度控制

#### 正则化技术 Regularization
	
- 定义：正则化是一个**防止过度**拟合的**回归技术**
	- 作用于 特征选择 阶段
	- 调整 **强噪音对应的特征** 的权重，从而在本轮训练不使用该特征。
- 基本思想：在训练时**限制模型**，防止模型的参数变得过大或过于复杂
	- 在损失函数中加入一个额外的项，用来**惩罚过于复杂的模型**。（去除无关或噪音特征）
	- 在普通线性回归中，目标是最小化loss函数
	
![[L1-L2.webp]]
![[regularization-comparation.png]]

**L1正则化（Lasso Regularization）**
- 对模型参数的绝对值求和来增加惩罚项。
- 特点：
	- 特别适合处理稀疏数据
**L2正则化（Ridge Regularization）**
- 对模型参数的平方和进行惩罚。
**弹性网正则化（Elastic Net Regularization）**
- 也叫**L1/L2混合正则化**
- 弹性网正则化结合了L1正则和L2正则化的优点。
![[regularization-plot-comparation.png]]


#### Dropout
- 定义：一种用于DL模型训练中的**正则化技术**。
- 目的：
	- 减少过拟合，提高模型泛化能力
	- 减少神经元之间的共适应性
- 原理：
	- 在每次训练迭代中，随机丢弃一部分神经元，不参与当前计算。
		- 丢弃比例由超参数 dropout rate决定，通常在0.2-0.5之间
	- 使得模型训练过程中不会过度依赖某些神经元，从而减少过拟合。
- 缺点：
	- 增加训练时间
		- 有效计算量增加
	- 可能影响模型学习效率
		- 过度使用阻碍模型学习到足够的特征

#### **早停（Early Stopping）**
[[2.x 模型评估与调优#早停 Early Stopping]]
- 早停也是一种正则化技术。
- 原理：
	- 在验证误差开始增大时提前停止训练
	- 避免模型训练得过长，防止开始记住训练数据中的噪声

---
## 高级优化技术
- 自动化机器学习 (AutoML)
- 神经架构搜索 (NAS)
- 迁移学习
	- 预训练模型微调
- 半监督学习
- 对抗训练

### 迁移学习
- 定义 🧠
	- 从一个任务中学到的知识（通常是通过预训练的模型）应用到与之相关的另一个任务中
	- 从而加速新任务的学习，并提高模型的性能。
- 核心理念
	- 利用已有的知识，特别是对大规模数据集上训练的DL模型，将其应用到目标任务上。
- 用好迁移学习的关键要点：
	- <font style="background-color: #FFFACD; color:black; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">领域适用</font> 
		- 特征分布对齐
		- 领域不变特征学习
		- 自适应算法设计
	- **预训练模型**
		- 预训练模型是先在大规模数据集上训练得到的模型，然后将其在目标任务上进行微调(fine-tuning)
		- 迁移学习常常依赖于预训练模型
	- 冻结早期层，仅调优后期层
	- 调整学习率
	- 使用数据增强
- 优点
	- 减少训练时间
	- 提高效率和性能
	- 在小数据集上有高准确率
- 主要类型
	- <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">同源迁移</font> ： 
		- 描述：
			- 源任务和目标任务属于同一个领域，但使用的**数据分布**可能略有不同，通常共享相同的：
				- 数据类型
				- 数据模态 (如都是图像)
		- 关注点：
			- 可迁移的模型结构
			- 可迁移的参数
		- 🌰：
			- 文本领域：
				- 使用互联网语料预训练BERT，然后在医学文献上微调来进行疾病诊断
			- 图像领域
				- 使用ImageNet预训练的ResNet50模型，进行猫狗二分类。
	- <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">异源迁移</font>：
		- 描述
			- **数据特性**上存在差异。通常使用不同类型的数据
				- 数据模态不同：图/文/音
				- 数据分布不同：来源/统计特性、采集方式
				- 领域不同：从日常生图到医疗影像
		- 挑战：
			- 数据差异 可能导致性能
		- 核心思想
			- 找到跨领域、跨模态/分布的**共同知识或结构**
			- 设计特定的方法来**缓解数据间的差异**性
		- 这是迁移学习最强大的形式
	- <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">多任务学习</font>：
		- 定义：
			- 同时学习多个相关任务，模型在多个任务上共享表示
			- **任务相关性**：
				- 多个任务共享一定的底层知识或特征表示
	- 领域适用：
		- 专注于处理不同数据分布之间的差异。
		- 目标：在同一领域上训练的模型能够很地应用于另一个领域
### 预训练模型
- 定义
	- 在大规模数据集 (如ImageNet, COCO, Wikipedia)上预训练过
	- 学习到通用的特征和模式
	- 具有较强的泛化能力
- 重要用途
	- <font style="background-color: #FFFACD; color:blue; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">微调 Fine tuning</font>：
		- 冻结预训练模型的底层层（通常是权重不变），只更新顶层（针对新任务的层）
		- 然后在目标任务的数据集上进行进一步训练。
			- 迁移学习的一种重要实现方式
	- <font style="background-color: #FFFACD; color:blue; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">特征提取 Transfer Learning</font>
		- 使用预训练模型的最后一层或多个层的输出作为固定编码器，将新数据编码成特征表示
		- 然后训练一个从头开始构建的小网络模型来利用这些固定的特征。
			- (也就是仅复用原模型参数，而不以其为基座再训练，这是和微调的本质区别)
- 优势
	- 节省计算资源
	- 提高性能
	- 训练时间短
- 常见的预训练模型
	- CV领域
		- VGG16/VGG19: 较早的深度卷积神经网络模型，常用于图像分类
		- ResNet: 一种通过引入残差连接解决深度神经网络训练困难的模型
			- ResNet50
			- ResNet101
			- ResNet152
		- InceptionV3: google. 具备多尺度特征提取能力
		- EfficientNet： 高效卷积神经网络。使用复合缩放方法优化网络结构，减少计算量
		- DenseNet：通过直接连接每一层的输出，以解决信息流和梯度消失的问题
	- NLP
		- BERT： 通过双向编码器表示学习文本特征
		- GPT
		- RoBERTa： 改进版的BERT，采用更多训练数据和时长，进一步提升性能
		- T5
		- XLNet：通过自回归和自编码结合的方式提升性能
	- 语音处理
		- WaveNet：Google提出
		- DeepSpeech：基于深度学习的自动语音识别ASR模型
	- 多模型模型
		- CLIP: Openai提出。可处理图像和文本输入，进行图像描述、搜索等任务
		- BLIP：擅长图像理解和生成
	

---
## 资源优化
- 计算加速
	- GPU/TPU利用
	- 混合精度训练
- 模型压缩
	- 量化 (8-bit/4-bit)
	- 知识蒸馏
	- 剪枝

