
<h1 style=" text-align: center; font-size: 3em; font-family: 'Georgia', serif; color: #2c3e50; margin: 0.5em 0; padding: 10px 0; border-top: 4px solid #3498db; border-bottom: 4px solid #3498db; text-transform: uppercase; letter-spacing: 3px;"> 神经元和神经网络</h1>



![](https://cdn.nlark.com/yuque/0/2025/png/295096/1743990126952-2e9f4693-957a-47ce-9b96-0ad861223620.png)
## 神经元 (Neuron)

[What Is The Relation Between Artificial And Biological Neuron?](https://saurabhnativeblog.medium.com/what-is-the-relation-between-artificial-and-biological-neuron-18b05831036)


### 生物学中的神经元
 ![[biological-neuron.png]]
- 定义：接收刺激并将其转化为电信号，并能将该信号传递给其他神经元或组织细胞.
- 三大基本结构：
	- **树突 (dendrite)**: 接收来自其他神经元的信号
	- **胞体 (soma)**: 整合信号并产生反应
	- **轴突 (axon):** 将电信号传导出去，传递给下一个细胞
- **突触 (Synapse)**：两个神经元之间的信息传递"接点"
- 电信号 (electrical impulse)：神经元内部使用电信号传播
- 化学信号：神经元之间信息传递依赖 **神经递质（neurotransmitters）**

### 人工神经元

![[artificial-neuron.png]]
- 定义：神经网络的最小单位，本质是一个==函数组合==。

#### 工作原理

1. 接收输入, 每个输入一个权重（weight）：
	- 输入 $$x_1, x_2, \dots, x_n$$
	- 权重  $$w_1, w_2, \dots, w_n$$
    
2. 加权总和：
    $$z = w_1x_1 + w_2x_2 + \dots + w_nx_n + b$$
    
    - b 是**偏置 bias**
    
3. 通过激活函数（activation function）处理输出：    $$y = \sigma(z)$$
4. 输出


#### 激活函数
>  **激活函数**是神经网络具备表达非线性映射能力的关键。
	
- 作用： 引入非线性
	- 非线性(Non-Linearity)：是模型学习复杂数据模式的关键。
- 激活函数是否被激活 的意义
	- 每个神经元在经过激活函数处理后是否会产生一个有效的输出（即非零输出或有效的激活值）。
	- 这个输出是下一层神经元的输入，进而影响最终预测。

[Activation Functions for Deep Learning](https://medium.com/@toprak.mhmt/activation-functions-for-deep-learning-13d8b9b20e)

<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">常见激活函数</font>

1. **Sigmoid函数**![[sigmoid.png]]
	- 🎯：将输入值压缩到==0和1之间==
	- 特点：
		- 所有输出神经元的值非零且和为1，无严格零输出。
		- 确保了最多只有一个神经元具有最高的激活值（==赢家通吃==），而其他的神经元将具有较小的激活值或不激活
	- 应用：常用于二分类问题
	- 公式：$$\sigma(x) = \frac{1}{1 + e^{-x}}$$
	
2. **ReLU（Rectified Linear Unit）函数**：![[relu-activation.png]]
	- 定义：输入大于0时输出输入值，否则输出0。
	- [A practical Guide for ReLU](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7)
	- 优点：
		-  计算简单，广泛使用
		- 有助于缓解梯度消失问题。
	- 特点：
		- 输入为负时输出严格为零，部分神经元被关闭(不激活)。
	- 公式：$$\text{ReLU}(x) = \max(0, x)$$
3. **Tanh函数**：![[tanh-activation.png]]
	- 🎯：将输入值压缩到==-1和1之间==，类似于Sigmoid，但输出范围更大。
	- 应用：常用于循环神经网络（RNN）。
	- 特点：
		- 无论输入正负，输出均在(-1,1)范围内，无非零抑制。
	- 公式：$$\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

4. **Softmax函数**：![[softmax-fun.png]]
	- 🎯：将一组输出转换为概率分布，使得每个输出值介于0和1之间，且所有输出值的和为1
	- 特点：
		- 无论输入正负，输出均在(0,1)范围内，无非零抑制。
	- 应用：常用于多分类问题
	- 公式：$$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$$
## 神经网络 (Neural Network)
![[neural-network-illustration.png]]

### 神经网络基本结构

#### **输入层（Input Layer）**
- 作用：接收**原始数据 (Raw Data)** 作为神经网络的第一个输入
- 神经元数量：层神经元的数量**严格等于**训练样本中的**特征数量 (Number of Features)**
- 输入层的神经元 ==通常不包含权重和偏置==
- 数据预处理：发生在数据传入输入层之前

#### **隐藏层（Hidden Layers）**
- 不是输入输出层的神经层 都是隐藏层
- 层叠 (Stacking): 深度神经网络的深度指的就是网络包含多个隐藏层。
	- 通过堆叠多个带有非线性激活函数的隐藏层，网络能够学习数据中更高层次、更抽象的特征表示。
	- 深度带来的挑战：
		- 训练难度，如梯度消失/爆炸、过拟合
		- 资源的巨大耗费
- 参数，
	- 每一个神经层都有
		- 权重矩阵
		- 偏执向量
	- 可学习性：在整个训练过程中被优化调整
- 神经元死亡 Neuron Death (主是Relu函数)
	- 神经元输入在训练过程中一直为负，且学习率足够大，其权重可能被更新到使得该神经元的输入永远为负，导致神经元死亡
	- 死亡后不再对任何输入做出响应，不再学习
	
<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">隐藏层类型</font>
1. **全连接层 (Fully Connected Layer / Dense Layer):**
	- 也叫 致密层(Dense Layer)
	- 特点
	    - 在这个层中，每个神经元都与上一层的**所有**神经元连接，并且还有一个偏置项。
	    - 输出计算通常涉及权重矩阵乘法和偏置加法。
    - **优点:** 
	    - 容易实现，能够学习输入和输出之间复杂的非线性关系。
    - **缺点:** 
	    - 参数量可能非常大（如果层数深、神经元多），容易过拟合，模型复杂。
2. **卷积层 (Convolutional Layer):**
    - 主要用于**卷积神经网络 (Convolutional Neural Network, CNN)**
    - 特别擅长处理具有**结构化**或**空间相关性**的数据，如图像。
    - 卷积层包含卷积核 (Filters/kernels)，它们在输入数据上滑动，进行局部区域内的加权求和操作。这种结构能够**自动学习特征**（如图像的边缘、纹理、形状）。
    - 常伴有激活函数、池化 (Pooling) 等操作。
    - 具有参数共享 (Parameter Sharing) 的特性，大大减少了模型复杂度，使其能够处理高分辨率输入。
3. **循环层 (Recurrent Layer):**
    - 主要用于**循环神经网络 (Recurrent Neural Network, RNN)**
    - 特别擅长处理具有**顺序性**或**时间依赖性**的数据，如文本、时间序列。
    - 循环层中的神经元具有**可记忆**的特性，其输出不仅取决于当前输入，还取决于**前一时刻的隐藏状态**。这使得模型能够建立长期的依赖关系。
    - 常见的变体
	    - LSTM (长短期记忆网络) 
	    - GRU (门控循环单元)。
    - **优点:** 能够处理变长的输入序列，捕捉时间动态。
    - **缺点:** 可能存在梯度消失/爆炸问题，处理非常长的序列时效果可能不佳。
- 
#### **输出层（Output Layer）**
- 核心作用：产出神经网络的**最终预测或输出结果**
- 神经元和激活函数的数量：**由具体的任务目标决定**
	- 回归任务 Regression：通常只有一个输出神经元，且不需要激活函数
	- 二分类任务 Binary Classification：只有一个输出神经元，sigmod激活函数
	- 多分类任务：通常有N个输出神经元(N为类别数)，softmax激活函数
	- 多标签分类任务 Multi-label Classification: 每个神经元对应一个标签。sigmoid激活函数
- 预测解释：输出必须被适当地解释以匹配任务需求

### 网络参数
- 权重矩阵
- 偏置向量

### 超参数
- 在训练开始之前设置的参数。控制训练过程本身或网络的结构
- 常见的超参数有：
	+ **学习率（Learning Rate）**：决定了每次更新时参数的调整步长
	+ **批量大小（Batch Size）**：在一次更新中所使用的样本数量。
	+ **迭代次数（Epochs）**：整个训练集被训练一次的次数。
	+ **优化算法（Optimizer）**：用于更新模型权重的算法。
	+ **正则化参数（Regularization）**：控制模型复杂度的超参数，防止过度拟合
	+ **网络结构的超参数**：例如深度学习中的层数、每层的神经元数量、卷积核的大小等。
	
- 超参数调优方法：
	- 经验
	+ **网格搜索（Grid Search）**：通过穷举法测试预设的超参数组合，选择最佳的组合。
	+ **随机搜索（Random Search）**：随机采样超参数空间并评估模型效果。
	+ **叶斯优化（Bayesian Optimization）**：通过构建概率模型来指导超参数搜索过程，更加高效。
	
### 网络初始化 (Network Initialization)

- 权重矩阵通常需要随机初始化。选择合适的初始化方法很重要
	- 初始化不当可能导致 梯度消失 或 梯度爆炸
- 方法：
	- 零初始化 Zero Initialization
	- 随机初始化 Random Initialization
	- Xavier/Glorot 

### 深度学习模型
- FNN，如MLP 多层感知机
- RNN
- CNN
- Transformer
- Autoencoders

### 主流深度学习库
- Tensorflow
	- 最流行
		- 有庞大的社区
	- 谷歌2015发布，现用于谷歌research和production需求
- PyTorch
	- 基于 Lua 的 Torch 框架
	- 支持运行在gpu的机器学习算法
	- 快速且原生流畅
	- 2016年发布，成为tf的重要竞品
	- Facebook (Meta) 支持和使用
	- PyTorch 和 tf 有陡峭的学习曲线
- Keras
	- 构建DL模型的高阶API
	- 最简单的API，快速开发的首选库
		- 少量代码就能构建复杂深度学习网络
		- 不适合对节点和层进行  **细粒度控制** 
	- 可在底层库如tf上运行
		- ==必须先安装tf==
	- 谷歌也支持
	- Keras有哪两类模型？
		- Sequential 
		- Functional API
    
## 神经网络的训练过程

![[dnn-forward-and-back-propagation.png]]

### 向前传播 (Forward Propagation)
- 定义:
	> 计算网络输入到输出的整个过程 
	> 给定输入，逐层计算神经元激活值，最终得到输出层结果。
	
- 作用：
	- 模型训练的基石
	- 模型预测
	- 函数组合的实现：
		- 向前传播的过程，也是输入数据，经过多个复杂函数的一步步计算，得到输出
	
### 反向传播 (BackPropagation)
- 必备知识：[[2.4 梯度下降和损失函数]]
- 定义：
	>计算损失函数关于每个网络参数(权重矩阵和偏置向量)的梯度。
	>它是深度神经网络核心的优化算法思想。
- 🧠：**利用误差，从输出层往输入层“逆推”问题来源，并调整参数，使模型的预测更准确**。
- 目标：
	- 计算Loss函数对所有参数的梯度 (求导)
	- 然后通过梯度下降更新参数
- 工作流程：
	1. 向前传播计算预测值
		- 输入：x
		- 隐藏层：$z=Wx+b, a = \sigma(z)$, 其中$\sigma$是激活函数
		- 输出：$\hat{y} = W'a+b'$ 
			- a 是 隐藏层输出的激活值
			- W' 和 b‘ 是输出层的权重矩阵和偏置向量
		- ⚠️细节：
			- 向前传播结束的标志是 输出层输出预测结果 $\hat{y}$
			- 反向传播开始，是基于**正向传播产生的预测结果** $\hat{y}$，然后与真实标签 y 比较，计算损失函数值
		
	2. 计算损失函数值
		- 如，使用MSE: $$\text{Loss} = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2$$
	3. 反向传播计算梯度![[gradient-descent.png]]
		- 🎯 目标： 反向传播的目的是通过梯度下降法调整网络的权重和偏置
			- 输入层通常不包含权重矩阵，不涉及参数更新操作
		- Loss函数计算输出层梯度：
			-  Loss函数 对 $\hat{y}$ 求导:  $\frac{\partial \text{Loss}}{\partial \hat{y}} = \hat{y} - y$
			- 上述示例中，输出层无激活函数
			- $\hat{y}$ 对 W' 求导：$\frac{\partial \hat{y}}{\partial W{\prime}} = a$, $a$是上一层的输出
			- ==根据链式法则， Loss函数对 W‘ 求导==：$\frac{\partial \text{Loss}}{\partial W{\prime}} = (\hat{y} - y) \cdot a$
			- 同理，Loss函数对 b‘ 的求导：$\frac{\partial \text{Loss}}{\partial b{\prime}} = \hat{y} - y$
		- 计算隐藏层梯度
			- 损失函数关于隐藏层的激活值 a 求导：$\frac{\partial \text{Loss}}{\partial a} = \frac{\partial \text{Loss}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a} = (\hat{y} - y) \cdot W{\prime}$
			- 根据激活函数的导数，计算加权输入 z 的梯度：$\frac{\partial \text{Loss}}{\partial z} = \frac{\partial \text{Loss}}{\partial a} \cdot \sigma{\prime}(z)$
				- $\sigma{\prime}(z)$ 是激活函数 $\sigma$ 对 z 的导数
			- 计算隐藏层权重和偏置的梯度:
				- 对 W 的梯度：$\frac{\partial \text{Loss}}{\partial W} = \frac{\partial \text{Loss}}{\partial z} \cdot x^T$
					- $x^T$ 代表的是输入 x 的转置
					- **为什么是** $x^T$? 
						- ==矩阵求导的维度对齐==
				- 对 b 的梯度：$\frac{\partial \text{Loss}}{\partial b} = \frac{\partial \text{Loss}}{\partial z}$
		- 输出：
			- 损失函数对每个模型参数的梯度
				- 一个数组或张量，loss函数对每个参数和偏2导数，数值
	- 统一参数更新 
		- ⚠️细节：
			- 每次反向传播并不总是一次就找到局部最小值，而是每次梯度下降一定的步长 ($\text{学习率}*\text{梯度}$)
				- **多次迭代**:  直到接近一个局部最小值或停留在鞍点附近
			- 每次反向传播只会最终计算梯度，然后同时更新所有参数
				- 并不会每一层计算一次梯度后变立即计算并更新
		- 参数更新公式：$$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta_t)$$
			-  $\theta$：模型参数，如 W' 和 b‘
			- $\eta \cdot \nabla_\theta J(\theta_t)$:  **步长** = 学习率 (超参数) ✖️ 损失函数对参数的梯度
	
- ‼️ 既然梯度是不同参数的偏导数集合, 为什么不只计算一次梯度, 然后取每个参数对应的偏导数, 再✖️学习率？
	- 每个参数的影响不同
	- 参数的更新是逐个进行的
	- 每一层的梯度是相互依赖的，逐层更新可能会导致前一层的梯度被破坏，影响后续层的计算
	
#### 学习率 (Learning Rate)
![[learning-rate.png]]
- 决定每次参数更新的步长。（梯度下降的步长）
- 它是最重要的**超参数**之一
- 挑战：
	- 学习率过大可能导致无法收敛，过小则使训练速度变慢。
- 改进：
	- 手动调整学习率
	- 使用自适应学习率方法，(如 AdaGrad、 RMSProp、 Adam)可提高训练效率
- ⚠️ 细节：
	- 即使优化算法支持自适应，学习作为超参数，也需要指定基础值。
		- 优化算法会基于这个基础学习率调整

#### 优化器 Optimizer
- 定义：
	> 寻找某个==目标函数的最优解==, 目标函数通常是 Loss 函数。  
- ⚠️：传统机器学习 通常只需要梯度下降和二阶方法，不需要复杂优化器  
- **输入**：模型参数
- **输出**：优化函数的计算结果，用于衡量当前输入参数的优劣。
- **优化问题的核心组成**：
	+ **目标函数 Objective Function**
		+ 在机器学习中一般是loss函数
	+ **决策变量 Decision Variables**
		+ 优化问题中可控的变量，如模型参数
	+ **约束条件 Constraints**
		+ 作用：确保优化过程不会违反特定的规则或限制
		+ 机器学习的约束可能包括权重的范围限制与正则化条件
	+ **最优解 Optimal Solution**
### 优化算法
- 梯度下降 （Batch Gradient Descent）
	- 原理：每次使用全部训练数据 来计算一次梯度，然后更新参数
	- ⚠️：
		- 梯度下降本身就是优化器的一种
	- 公式：$$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta_t)$$
- 小批量梯度下降（Mini-Batch Gradient Descent）
	- 原理：把训练数据分成小批（比如每次用 32 个样本），每个 mini-batch 计算一次梯度并更新参数。
	- ⚠️细节：
		- 每个 mini-batch 的计算是 **独立的正向传播 + 反向传播**。
		- **每次反向传播只能更新一次参数**，但每个epoch会经历多次正、反向传播
	- 优点：
		- 在效率和稳定性之间找到折中
		- 可以利用GPU并行
	- 公式： $$\theta \leftarrow \theta - \eta \cdot \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta; x^{(i)}, y^{(i)})$$
- 随机梯度下降 SGD
	- 相比于批量梯度下降，SGD在每次迭代中只使用一个样本来计算梯度，
	- 🎯 目标：
		- 使得更新更加频繁，且有更多的随机性
		- 可以帮助模型跳出局部最小值
- 动量 Momentum
	- 通过引入动量，优化器可以在更新参数时考虑之前梯度的记忆
	- 作用：
		- 加速收敛
		- 避免在小的局部最小值附近震荡
	- 公式：$$v_{t+1} = \beta v_t + (1 - \beta) \nabla_\theta J(\theta_t)$$ $$\theta_{t+1} = \theta_t - \eta v_{t+1}$$
- Adagrad
	- 原理：自适应学习率的算法，基于历史梯度的平方 来调整每个参数的学习率。
		- 较大的学习率会变小，较小的学习率会变大
	- 优点：
		- 自适应学习率，能够处理稀疏数据。
	- 缺点：
		- 学习率会逐渐减少，因此可能会提前停止学习，造成收敛不完全。
	- 更新公式：$$G_t = G_{t-1} + g_t^2$$$$\theta \leftarrow \theta - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot g_t$$
		- $G_t$ 是梯度的平方的累加和
		
- Adam (Adaptive Moment Estimation)
	- 每次不仅更新梯度信息，还考虑了梯度的一阶矩(均值)和二阶矩(方差)，从而自适应地调整每个参数的学习率
	- 特点：
		- 目前最流利的优化算法之五
	- 优点：
		- 结合动量和自适应学习率的思想，特别适合深度神经网络的训练
		- 快速收敛，默认超参数设定通常工作得很好
	- 缺点：
		- 在某些情况下(如非常稀疏的数据集)，可能有一些不稳定
	- 公式：$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}t = \frac{v_t}{1 - \beta_2^t}$$$$\theta{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$
		- $m_t$和$v_t$分别是梯度的一阶矩和二阶矩估计
		- $\beta_1$, $\beta_2$是衰减率
		- $\epsilon$是防止除零的小常数
- RMSprop
	- 原理：通过对梯度的平方进行指数误差平均，来调整每个参数的学习率
	- 优点：
		- 学习率根据==**历史梯度调整**==，**避免了梯度爆炸和消失**的问题
		- 特别适用于非平衡的目标函数，如RNN
	- 公式：$$E[g^2]t = \rho E[g^2]{t-1} + (1 - \rho) g_t^2$$ $$\theta \leftarrow \theta - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \cdot g_t$$
		- $g_t$ 是当前的梯度。
		- $E[g^2]_t$ 是梯度平方的指数衰减平均。
		- $\rho$ 是衰减率，通常取值在 0.9 左右。
		- $\epsilon$ 是一个小常数，避免除零错误。
	
### 模型更新的频率 Update Frequency
> 模型在训练过程中进行<font style="background-color:yellow; color:black">参数更新的频率</font>。

- 模型更新频率由 **梯度下降算法** 和 **批次大小** 来决定。
	- 批大小：一个epoch分n批执行，就会有n次反向传播，通常也会更新模型n次。
	- 在一些高级优化器中，每个batch的计算都会使用当前的梯度来更新模型参数。这些优化器会在每次计算梯度后调整学习率，并根据梯度的历史进行自适应调整。

- 更新在几个层面：
	+ **Batch Update** 在每个小批量数据（mini-batch）上进行一次参数更新
	+ **Epoch Update** 在整个训练集上迭代一次之后，更新模型的参数。
	+ **Online Update** 对于每个训练样本都进行一次更新


## 参考资料

- [【官方双语】深度学习之神经网络的结构 Part 1 ver 2.0_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.788.recommend_more_video.0&vd_source=d2c6cad4e8b48a4a5ab3df7cb838685b)
- [【官方双语】深度学习之梯度下降法 Part 2 ver 0.9 beta_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Ux411j7ri/?spm_id_from=333.788.recommend_more_video.0)
- [【官方双语】深度学习之反向传播算法 上/下 Part 3 ver 0.9 beta_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV16x411V7Qg/?spm_id_from=333.788.recommend_more_video.-1&vd_source=d2c6cad4e8b48a4a5ab3df7cb838685b)
- [neuralnetworksanddeeplearning.com | bitJoy](https://bitjoy.net/category/0%e5%92%8c1/neuralnetworksanddeeplearning-com/)

