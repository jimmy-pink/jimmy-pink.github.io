<h1 style=" text-align: center; font-size: 3em; font-family: 'Georgia', serif; color: #2c3e50; margin: 0.5em 0; padding: 10px 0; border-top: 4px solid #3498db; border-bottom: 4px solid #3498db; text-transform: uppercase; letter-spacing: 3px;">深度学习概述</h1>

## 深度神经网络模型分类
[[深度学习模型分类.canvas|深度学习模型分类]]

![[dl-models-classification.png]]

### 前馈神经网络 (FNN)
![[fnn-illustration.png]]
- 定义:（Feedforward Neural Network, FNN）
	> 信息单向流动（输入层→隐藏层→输出层）, 无反馈连接
- 特点：
	- 全连接结构
	- 没有循环，信息单向流动
	- 适合处理固定大小的输入数据
	- 适合结构化数据
- 实现：
	- 多层感知机 MLP
- 关键点：
	- 激活函数，如ReLU/Sigmoid
	- 反向传播
	- 梯度消失问题
- 缺点：
	- 不适合处理序列数据，或具有长期依赖关系的数据
	- 无法处理空间数据和时间序列
	- 参数量大，计算成本高
- 应用：
	- 图像分类
	- 回归任务
	- 结构化数据预测

### 卷积神经网络 (CNN)
![[cnn-schematic.png]]
[[3.3 卷积神经网络 CNN]]
- 特点：
	- 局部连接和权值共享，减少参数量
	- 自动提取空间特征
	- 具有平移不变性
	- 适合处理图像等高维数据

### 循环神经网络 (RNN)
![[rnn-illustration.png]]
[[3.4 循环神经网络 RNN]]

#### 长短期记忆网络 (LSTM)
![[rnn-lstm-workflow.png]]
[[3.4 循环神经网络 RNN#长短期记忆网络 LSTM]]

### 生成对抗网络 (GANs)
![[gans-1.png]]
- 定义：（Generative Adversarial Networks, GANs）
	- 有两部分组成
		- 生成器 Generator
		- 判别器 Discriminator
	- 生成器生成假数据，判别器区分真假数据
	- 二者通过对抗训练提升模型能力
- 特点：
	- 生成器学习生成样本，判别器学习判断样本真假
	- 可以生成相对真实的高质量图像、视频
	- 无需显式定义数据分布
- 缺点：
	- 训练过程不稳定，容易出现模式崩溃(Model Collapse)问题
	- 训练过程需要精心调参
	- 评估生成质量较困难
- 应用场景：
	- 图像生成
	- 风格转换
	- 图像修复
	- 虚拟现实

### Transformer
[[3.5 Transformer]]
![[transformer-simple.png]]


### 自编码器 (Autoencoders, AE)
[[3.6 Autoencoder, AE]]
![[auto-encoder.png]]


### 图神经网络 (GNN)
- 定义：
	- 处理图结构数据
	- 通过信息传播机制处理图节点之间的关系
- 特点：
	- 节点之间通过边进行信息交换
	- 能处理非欧几里德结构的数据，适用于复杂关系的学习
	- 具有归纳学习能力
- 缺点：
	- 计算复杂度较高
	- 图的结构可能需要额外的预处理
	- 对大规模图处理效率较低
- 应用场景：
	- 社交网络分析
	- 分子结构预测
	- 推荐系统
	- 交通预测

### 深度强化学习模型 (Deep RL)
- 定义：
	- 强化学习是使代理在环境中通过试错学习最优策略的算法
	- 而深度强化学习，则结合深度学习方法，利用深度神经网络作为RL的函数逼近器
- 特点：
	- 基于奖励机制进行学习
	- 适用性强，能够在没有明确标签的情况下学习，适应复杂环境
	- 可以处理高维状态空间
- 缺点：
	- 训练过程不稳定
	- 样本效率低
	- 超参数敏感
- 应用场景：
	- 游戏AI
	- 机器人控制
	- 自动驾驶
	- 资源调度
	
### 多模态模型 (Multimodal Models)
- 定义：
	- 能够同时处理和融合多种类型数据
		- 如文本、图像、视频、声音
- 特点：
	- 结合不同模态的数据进行联合学习，提升模型泛化能力
	- 具有跨模态学习和推理的能力
	- 能够处理复杂的多模态任务
- 缺点：
	- 需要大量的多模态数据和复杂的预处理
	- 网络结构复杂，训练难度大
	- 计算资源需求高
- 应用场景：
	- 图文检索
	- 视觉问答
	- 视频分析
	- 多模态生成

### 预训练模型 Pre-Trained Models
- 定义
	- 在一个大规模、特定任务的数据集上已经训练好的机器学习模型
- 目标：
	- 学习能通用，能够捕捉到数据中基本模式和特征的知识表示 或 特征提取能力
- 用途
	- 在新任务上[[3.x 模型优化#迁移学习|迁移学习]]
	- 用于 **特征提取**
- 🧠
	- 预训练模型就像是已经学会了“识字”、“理解基本语法”的“学徒”，然后再去学习具体的“写作”、“翻译”、“阅读理解”等具体技能（目标任务）
- 关键特性
	- 学习通用知识
	- 节省时间和计算资源
	- 提升性能
	- 可解释性和可复用性好
- 用于特征提取时的优势
	- 不需要额外训练
	- 已学习特征的有效利用
	- 非常适合有限资源 
		- 计算资源
		- 训练数据
- 用于模型调优时
	- 性能提升
	- 可调节预训练权重来匹配数据
	- 迁移学习
	- 可解冻 模型基础的顶层 以进行特征提取
- 常见预训练模型
	- BERT： 在海量文本数据上预训练，学习文本的深层语义表示
	- GPT： 从单向语言模型发展而来，擅长生成文本
	- T5：将所有NLP任务都视为文本到文本的转换
	- VGG/RestNet/Inception/MobileNet/EfficientNet
		- 计算机视觉预训练模型
		- 都是在大规模图像分类数据集(如ImageNet)上训练，用于特征提取。