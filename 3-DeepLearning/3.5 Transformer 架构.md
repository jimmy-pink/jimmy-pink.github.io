<h1 style=" text-align: center; font-size: 3em; font-family: 'Georgia', serif; color: #2c3e50; margin: 0.5em 0; padding: 10px 0; border-top: 4px solid #3498db; border-bottom: 4px solid #3498db; text-transform: uppercase; letter-spacing: 3px;">Attention Models</h1>

## Transformer

![[transformer-full.png]]

### 什么是Transformer？
- 定义：
	- 基于自注意力机制的深度学习模型
	- 一种用于处理==序列数据==的DL模型
	- 由Vaswani等人在2017年的论文《[Attention Is All You need](https://arxiv.org/abs/1706.03762)》提出
	- 通过 **自注意力机制（Self-Attention）** 来捕捉序列中不同元素（如单词、字符、特征等）之间的相互关系和上下文依赖性。
	- 最初用于NLP，后被广泛应用于多种任务
- 特点：
	- 完全依赖==注意力机制== (Attention Mechanism) 来捕捉输入序列和输出序列之间的依赖关系
- 优点：
	- **并行性**： 
		- 可同时处理输入序列的不同部分，提高计算效率
	- **长距离依赖**： 
		- 自注意力机制能够捕捉长距离的依赖关系，而RNN在处理长序列时容易出现梯度消失和信息丢失的问题
		- 能够处理更长的输入序列
	- 有效处理变长序列 与 缺失数据
	- 强大的表示能力： 
		- Transformer在许多任务上取得了超越传统模型的性能
- 缺点：
	- 计算复杂度高，训练时间长
	- 内存消耗大
	- 需要大量训练数据
- 应用：
	- 机器翻译
	- 文本分类
	- 情感分析
	- 问答系统
	- 图像分类/Text-to-Image
	- 时间序列预测
	- 多模态任务
- 变体：
	- GPT
	- BERT
	- Adobe Photoshop Image Transformer
- 动手实战
	-  <font style="background:#F6E1AC;color:#664900">🌰 快速上手 </font>[ ✍️ Coursera Lab:  高级Transformers解决线性回归问题](https://github.com/jimmy-pink/colab-playground/blob/main/coursera-lab/Transformer-SimpleRegression.ipynb)
	- <font style="background:#F6E1AC;color:#664900">🌰 快速上手 </font>[ ✍️ Coursera Lab： 基于Transformer的文本生成DEMO](https://github.com/jimmy-pink/colab-playground/blob/main/coursera-lab/Transformer-TextGeneration.ipynb)
### 核心组件
#### 输入嵌入 (Input Embedding)    
- 将输入的词汇通过[[4.1 NLP 入门#单词嵌入 Word Embedding|Word Embedding]]转化为高维向量。每个词token映射成一个n维向量。
![[word-embedding-1.png]]

#### 位置编码 (Positional Encoding)
![[transformer-positional-embedding.png]]
- 作用：
	- 为输入token注入位置编码
	- 由于Transformer 不像RNN或CNN那样具有自然的时间或顺序信息，因此需要附加位置信息。  
- 位置编码 如何计算得到？通常采用**正弦和余弦函数**。
	- 基于正弦的位置编码 (奇数维)，公式：$PE(i, 2k) = \sin\left( \frac{i}{10000^{2k/d_{\text{emb}}}} \right)$
	- 基于余弦的位置编码 (偶数维)，公式：$PE(i, 2k+1) = \cos\left( \frac{i}{10000^{2k/d_{\text{emb}}}} \right)$
		- $d_{\text{emb}}$ 是input embedding的嵌入维度
		- i 是词在输入序列中的位置，通常从0开始
		- k 是位置编码向量的维度索引。 2k和 2k+1 分别对应位置编码的 奇数维 和 偶数维
#### Combined Input Representation
输入变量X：![[combined-input-representation.png]]
#### 自注意力机制 Self-Attention Mechanism
作用：
- ==计算每个token与其他所有token的相关性== 
- 捕捉**一个序列内**的全局依赖关系。  
- 广泛应用于Transformer模型的**编码器**部分。
**自注意力机制的输出通过多头注意力（Multi-Head Attention）进一步提升，将不同的相关性信息分开处理再合并。**


<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">注意力的计算步骤</font>：![[self-attention-mechanism.png]]
1. 输入表示：
	- 假设有一个输入序列X，它的维度是$(n,d)$
		- n是序列长度.  在nlp中，n一般表示输入句子的单词数，[[4.1 NLP 入门#单词嵌入 Word Embedding]]
		- d是每个元素的维度 (通常是词嵌入的维度, 上图中是5维)
	- 输入经过线性变换，得到查询(Query)，键(Key)， 值(Value)向量，公式：$$Q = X W_Q, \quad K = X W_K, \quad V = X W_V$$
		- 其中，$W_Q, W_K, W_V$ 是学习到的权重矩阵 ； 
2. 计算Attention  Scores
	- Attention Scores衡量的是查询向量(Query ) 与键向量(Key) 之间的相似度。
	- 常用的 **点积** 来计算相似度$$\text{Attention Scores} = Q K^T$$
		-  $K^T$ 是 **键（Key）** 矩阵的转置，表示查询与键之间的相似度。
3. 缩放 (Scaling)
	- 为了避免点积结果过大而导致梯度消失或爆炸，通常会对点积结果进行缩放。
	- 具体做法是将点积结果除以一个缩放因子：$$\text{Scaled Attention Scores} = \frac{Q K^T}{\sqrt{d_k}}$$
		- 其中，$d_k$ 是键向量的维度。这个缩放因子确保了点积不会变得过大。
4. 应用 Softmax：
	- 将缩放后的Attention Socres输入到Softmax函数中，以便将它们转化为概率分布。
	- 目的：**归一化注意力权重**。将Attention Scores转换为加权系数。公式：$$\text{Attention Weights} = \text{Softmax}(\frac{Q K^T}{\sqrt{d_k}})$$
5. 加权求和 Weighted Sum：
	- 最后，使用Attention Weights 对值(Value) 向量进行加权求和，从而得到最终的Attention Output。公式：$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
		- 其中：
			- Q（Query）代表查询向量。
			- K（Key）代表键向量。
			- V（Value）代表值向量。
			- $\text{softmax}$是归一化函数。
			- $d_k$ 是键向量的维度。

#### 交叉注意力机制 Cross-Attention Mechanism
- 作用
	- 在**两个不同序列**之间计算注意力。
	- 一个序列的元素 (如Query)，与 另一个序列的元素(如Key和Value) 进行交互
	- 通常用于Transformer模型的**解码器**部分。
- 关键特点
	- 输入和输出来自不同的序列
	- 计算注意力时， 查询序列会参考键值序列的元素

#### 前馈神经网络 FFN
> Feed-Forward Network， 它和 FNN (Feed-Forward Neural Network) 是同一个东西

![[multi-layer-fnn.png]]
- 在Transformer中的作用：
	- **进一步处理自注意力机制的输出**
		- 对每个位置的表示（即自注意力输出的每个 token 的向量）进行独立的非线性变换，从而引入更强的表达能力
	- 非线性变换与特征映射
		- FFN 通常由两个线性层（`Linear + ReLU + Linear`）组成，中间通过激活函数（如 ReLU、GELU）引入非线性。
	- **位置独立性（Position-wise）**
		- FFN 对序列中的每个位置独立处理（同一套参数共享）
- 特点
	- 通常会包含两个线性变换 和 激活函数(ReLU)
	- 位置独立
		- 前馈神经网络是 **位置无关** 的，即它对输入序列中的每个位置单独进行计算
- 在Transformer中的计算流程：
	1. 输入： 经过自注意力机制后，得到的每个位置的向量表示x, （形状为 $(L, d_{\text{emb}})$）。
	2. **第一层**：将输入通过一个全连接层，将维度扩展到 $d_{\text{ff}}$（例如 2048）。
	3. **激活函数**：通常使用 ReLU 激活函数。
	4. **第二层**：通过第二个全连接层将维度压缩回 $d_{\text{emb}}$。
	5. **输出**：得到的每个位置的向量 $y_3$，它仍然是一个形状为 $(L, d_{\text{emb}})$ 的矩阵。
	
#### 残差连接 Residual Connection
- 定义 ：（图中的Add就代表RC）
	- 用来帮助信息在网络中更容易流动。
- 原理：
	- 将输入直接加到输出中，而不是仅仅依赖于网络的计算结果。
- 公式：$$\text{Output} = \text{Layer Output} + \text{Input}$$
- 目的：
	- 缓解梯度消失问题
		- 在DNN中，梯度在反向传播时可能会逐渐消失。
		- 通过残差连接，梯度可以直接传递回去，避免这个问题
	- 加速收敛
#### 层归一化 Layer Normalization
- 定义：（图中的Norm代表归一化）
	- 将每一层的输入进行标准化处理，使得每个输入的均值上为0，方差为1.
- 目的：
	- 提高训练的稳定性
		- 防止某些层的激活值过大或过小
	- 加速收敛
		- 它让每一层的输入分布保持一致，避免了训练中出现梯度爆炸和梯度消失问题
	
	
### 工作流程 

Transformer的工作流程可以分为两个主要阶段：
#### 编码阶段 Encoder
- 接收输入序列，通过自注意力机制和FFN提取输入序列的表示
- 可以被堆叠多个相同的层，形成一个Encoder Stack (一般是6层)
#### 解码阶段 Decoder
- 接收目标序列，生成输出序列。通过以下机制实现：
	- 自注意力机制：用于关注输入序列
	- 自回归注意力机制： 用于关注已生成输出序列
- Decoder 也可以被堆叠多个相同的层，形成一个Decoder Stack。
- 教师强制 Teacher Forcing
	- 在解码器中使用已知的目标序列（而不是模型预测的目标）
- ==Outputs Embedding ==
	- 将 目标序列 转换成 嵌入向量
		- 在NLP中，目标变量可能是Token (猜下一个word)，也可能是序列 (猜下一句话)
	- 解码器输入的一部分
- ==目标序列右移一位 outputs (shifted right)==
	- 原理：将目标序列右移一位
	- 目的：
		- 确保解码器只能访问前一个时间步的输出，将目标序列中的单词右移一位，然后输入解码器
		- 这样解码器在第t步时，接收到的每t-1步的输出作为输入，而t=0时输入则是`<SOS>`
	- 🌰 例子
```
		# Transformer架构的翻译任务： 输入德语 输出英文
		Input Sequence = [Le, chat, est, assis, sur, le, tapis] # 输入序列 德语
		Target Sequence = [The, cat, sat, on, the, mat] # 目标序列
		Shifted Target = [<SOS>, The, cat, sat, on, the]  # outputs (shifted right)
```
	

这两个阶段的结构和流程是高度并行化的，基于 **自注意力机制** 和 **位置编码** 的计算来进行上下文信息的传递。
每个编解码器都有若干相似组件，核心部分就是 
- 自注意力机制 
- FFN

<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">Transformer工作流程</font>：
![[transformer-full.png]]
1. 输入
	- **Input Embedding**
	- **Positional Encoding**
	- Combined Input Representation
		- 两个矩阵 相加 得到输入变量X
2. Encoder
	- 接收输入序列
	- 组件
		- Add & Norm, [[#残差连接 Residual Connection|残差连接]]和[[#层归一化 Layer Normalization|归一化]]
		- [[#自注意力机制 Self-Attention Mechanism|自注意力机制计算]]
		- [[#前馈神经网络 FFN]]
	- 输出：生成一个包含输入序列上下文信息的表示
		- 每个编码器层的输出都会被传递到下一层
3. Decoder
	- 接收输入，包括：
		- 来自Encoder的上下文表示
		- Shifted Target：目标序列的右移版本
	- 组件
		- 目标序列输入 Target Embedding
			- 目标序列会通过词嵌入转化为向量表示。
		- 自注意力机制
		- 编码器-解码器注意力（Encoder-Decoder Attention）机制
		- FFN
		- Add & Norm
	- 输出：生成目标序列的输出。

##  高级Transformer架构

介绍Transformer架构的变种及基于Transformer架构融合其他技术的高级系统架构。
```
拓展：基于Transformer的高级架构
├── 1. 压缩型Transformer（减少计算/内存）
│   ├── 目标：缓解O(n²)复杂度问题
│   ├── 方法分类：
│   │   ├── 稀疏注意力（局部/全局组合）
│   │   │   ├─ Longformer（滑动窗口+全局token）
│   │   │   ├─ BigBird（随机+滑动+全局）
│   │   │
│   │   ├─ 低秩近似：Linformer（Key低秩投影）
│   │   ├─ 分块处理：Reformer（LSH分桶+局部注意力）
│   │   ├─ 内存优化：FlashAttention（IO感知算法设计）
│
├── 2. 高效注意力机制
│   ├── Linear Transformer：核函数改写注意力为线性复杂度
│   ├── Performer：使用随机正交特征近似Attention矩阵
│   ├── Sparse Transformer：可学习的稀疏连接模式
│
├── 3. 结构改进型
│   ├── 并联解码器：Transformer-XL（引入循环内存机制）
│   ├── 层次结构：Switch Transformer（混合专家模型）
│   ├── 无自回归：NAT（非自回归生成）
│
├── 4. 感知增强型
│   ├── 视觉Transformer（ViT）
│   │   ├─ 核心创新：图像分块映射为序列
│   │   ├─ 变种：Swin Transformer（层次化金字塔结构）
│   │
│   ├── 音频Transformer（wav2vec 3.0）
│
├── 5. 模态融合型
│   ├── VL-BERT（视觉-语言融合）
│   ├── CLIP （多模态对比学习）
│   ├── BLIP （生成式模型增强多模态学习）
│
├── 6. 超大规模应用
│   ├─ GPT系列（纯解码器结构）
│   ├─ BERT系列（编码器预训练）
│   ├─ PaLM（Pathways架构扩展）
│
├── 7. 新兴范式 (截至2024)
│   ├─ State Space Models（如Mamba）
│   ├─ 递归Transformer（递归参数复用）
│   ├─ 量子化Transformer（降低推理成本）

关键创新方向维度：
├── 高效性：注意力计算复杂度优化
├── 扩展性：长上下文处理能力
├── 多模态：跨模态特征对齐
├── 经济性：降低训练/推理成本
└── 任务适配：针对CV/ASR/NLP的专门化调整

```

#### Vision Transformers (ViT)
- 核心创新
	- 图像分块映射为序列
- 变种：
	- 层次化金字塔结构 Swin Transformer
#### 语音识别 (Wav2Vec 3.0)
- 语音识别模型（如 [Whisper](https://openai.com/research/whisper)）先将语音转换为频谱图，再通过 Transformer 编码器-解码器结构生成文本。

## 参考资料
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [NVIDA Blog: What’s a Transformer Model?](https://blogs.nvidia.com/blog/what-is-a-transformer-model/)
- [知乎：Transformer模型详解](https://zhuanlan.zhihu.com/p/338817680)
- [NLP with Attention Models by DeepLearning.AI](https://www.coursera.org/programs/sobma/learn/attention-models-in-nlp?authProvider=bancolombia&source=search)

