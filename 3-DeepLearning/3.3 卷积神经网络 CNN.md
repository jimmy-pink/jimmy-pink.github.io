<h1 style=" text-align: center; font-size: 3em; font-family: 'Georgia', serif; color: #2c3e50; margin: 0.5em 0; padding: 10px 0; border-top: 4px solid #3498db; border-bottom: 4px solid #3498db; text-transform: uppercase; letter-spacing: 3px;">CNN</h1>

## 卷积神经网络 CNN
![[cnn-diagram.png]]
- 原理：（Convolutional Neural Network, CNN）
	- 专门用于处理**网格状拓扑结构数据**， 如：
		- 图像：2D 网络像素
		- 视频： 3D风格像素 (在时间维度上)
		- 序列数据： 1D 网格(如，在NLP中，  将单词嵌入表示为2D或3D，或动态结构序列)
	- 等的深度神经网络。
	- 是一种特定类型的**深度前馈神经网络 Deep Feedforward Neural Network**
	- 通过
		- **局部感知** [[#局部感知 (Local Receptive Fields)|Local Receptive Fields]]
		- **权重共享** [[#权重共享 (Shared Weights)|Shared Weights]]
		- **层次化特征提取** [[#层次化特征 (Hierarchical Features)|Hierarchical Features]]
	- 自动学习数据的空间或时序模式。
- 🧠 核心思想：
	- 用一组可学习的===滤波器===在输入数据上进行 **滑动窗口操作 (Sliding Window Operation)**
	- 在每个窗口位置提取特征
	- 然后将这些特征组合起来得到最终输出
- 🌰 类比：
	- 人眼识别猫：1. 从局部细节(耳朵形状、胡须)；2. 局部特征组合，判断是否是猫
	- CNN：**从边缘→纹理→物体部件→完整物体**逐步抽象
- 🔧 核心组件：
	- 通过==卷积层==提取**局部特征**
	- 通过==池化层==减少**维度**
	- 最终通过==全连接层==进行**分类或回归**
- 特点：
	- 卷积操作使得网络可以自动学习空间层次结构
	- 权重共享和局部连接有效减少参数
	- 池化层减少空间维度，从而减轻计算负担
- 缺点：
	- 对序列数据处理不如RNN
	- 不能有效处理非结构化数据
- 应用场景：
	- 图像分类
	- 目标检测
	- 人脸识别
	- 语音识别
- 入门典例：==Fashion-Mnist==
	- [[Tensorflow 和 Keras 入门实战#Fashion Mnist - CNN]]

### 🔧 CNN的核心组件

#### 输入层 (Input Layer)
- 功能： **接收原始数据**
	- 如：一张图像通常表示为由像素值组成的3D张量，形状为`[Height, Width, Channels]`
		- `[28,28,3]` 表示一张$28*28$的RGB图像
		- `[28, 28, 1]` 表示一张灰度图
	- 也可以将张量转为1D向量输入

#### 卷积层 (Convolutional Layer)
- 定义：
	-  卷积：给图像加滤镜以突出图像的重要标志性特征。
- 功能：
	- 用**卷积核 (滤波器) 扫描输入**，**提取局部特征**。
	- 是CNN的核心计算单元
- 卷积核(滤波器)：
	- 一个小的权重矩阵，这些权重是共享的
	- 同一个滤波器在输入数据的不同位置检测的是同一个特征，只是提取到的具体**响应强度**不同。
	- 通常卷积层会包含多个(如32，64，128)并行的滤波器，每个滤波器关注一种不同的局部特征。
- 工作流程：
	- **卷积运算**： 
		- 滤波器 在输入数据的一个 局部区域 (滑动窗口区域) 进行元素相乘
		- 将所有相乘结果相加，得到该 滤波器 在当前位置的一个输出值
	- 重复卷积：
		- 将滤波器 平移 到输入数据的下一个位置，重复上述过程，直到覆盖输入数据的所有区域
	- 输出<font style="background-color: #FFFACD; color:blue; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">特征图</font>
		- 滤波器在整个输入上滑动的结果形成一个**特征图 (Feature Map)** 或 **卷积图 (Convolutional Map)**
- 例子 🌰：
	- 卷积核权重参数为：$$\begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 1 \end{bmatrix}$$
	- 计算过程![[cnn-filter-workflow.png]]
	- 输出：`3*3`特征图
 - 关键参数：
	- 步长 (Stide): 卷积移动的步幅，通常为1 
	- 填充 (Padding): 在图像边缘补0，控制输出尺寸。
		- 使用 same 填充通常可以保持输入和输出的空间尺寸相同
		- 使用 valid 填充则不添加填充，输出会小于输入
- 数学公式：$$(I * K)(i,j) = \sum_{m}\sum_{n} I(i+m, j+n) \cdot K(m, n)$$
	- `I`：输入矩阵（如图像像素）
	- `K`：卷积核（如3×3的权重矩阵）
	- `*`：卷积运算
	
#### 池化层/汇集层 (Polling Layer)
- 功能：
	- 下采样 (Downsampling): 
		- 将一组像素合并为一个代表值 （压缩图片）
		- 降维、减少计算量
	- 平移不变性 (Translation Invariance)：
		- 即使输入图片有小幅度变化（如平移、旋转），汇集层能保持特征不变。
		- 防止过拟合, 提高模型的泛化能力
- 工作流程：
	- 以**固定大小的矩形窗口**在特征图上滑动，并对窗口内的数值执行某个操作。
- 常见类型：
	- 最大池化 (Max Pooling)：取窗口内最大值 （保留显著特征）![[cnn-maxpooling.png]]
	- 平均池化 (Average Pooling)：取窗口内平均值 （平滑特征）  ![[cnn-avg-pooling.png]]
- 超参数：
	- 池化窗口大小 (e.g., `2*2`)
	- 步长 (e.g., 2) 通常步长等于窗口大小

#### 卷积基
由卷积层、池化层组成。

#### 激活函数 (Activation Function)
- 功能：
	- 引入非线性，使网络能拟合复杂函数
- 常用函数：
	- **ReLU**：$f(x) = \max(0, x)$（解决梯度消失，计算高效）。
	- **Leaky ReLU**：负区间保留微小梯度，避免神经元“死亡”。
	
#### 全连接层 (Fully Connected Layer)
- 功能： 
	- 将高层特征映射到最终输出 
- 工作原理：
	- 将前面卷积层和池化层提取到的 高级、抽象 的局部特征组合起来
	- 学习从 这些特征 到 最终输出 的全局映射关系 (最终决策)
- 位置：
	- 通常位于CNN末尾，连接所有神经元
- ⚠️：
	- 现代CNN，常用**全局平均池化 (GAP)** 替代全连接层以减少参数量

#### 输出层 (Output Layer)
- 位置：
	- 通常在全连接层之后
- 结构：
	- 二分类问题： 包含1个神经元，使用**Sigmoid**激活函数
	- 多分类问题：包含N个神经元(N为类别数)，使用**Softmax**激活函数
- 损失函数：
	- Binary Cross-Entropy
	- Categorical Cross-Entropy

### CNN的独特设计原则

#### 局部感知 Local Reception
- 核心思想：
	- 实际世界的信息，特别是图像，往往是 **==局部相关==** 的，一个像素的意义很大程序上取决于 **==邻域==** 的像素
- 优势：
	- 减少参数量
	- 捕捉局部模式，如边缘、角点
	
- <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">局部感受野</font> (Local Receptive Fields)
	- 定义：
		- 在CNN中，某一层的输出特征图上的每个像素在输入图像上所对应的区域大小。
	- 特点：
		- 感受野的大小决定了该层的感知范围
		- CNN每一层的感受野 由前一层的卷积核尺寸、步长、填充等超参数决定。
		-  通过不断堆叠卷积层，感受野会逐渐扩大，从而能够捕捉到更大尺度的特征。
	- 感受野为什么重要？
		- 感受野的大小对卷积神经网络的性能和特征提取能力有重要影响。
			- 较小的感受野可以捕捉到细节信息，适合处理图像中的小细体或细微纹理
			- 较大的感受野可以捕捉到更大尺度的特征，适合处理图像中的大物体或全局结构
	
#### 权重共享 (Shared Weights)
- 原理：
	- 同一个卷积核在整张图上滑动，参数复用 (内部权重保持不变)
- 效果：
	- 卷积核学习到的模式 (边缘方向、纹理、形状、部件) 可以在输入数据中的任意位置被检测到。
- 优势：
	- 大幅降低计算成本，增强平移不变性。

#### 层次化特征 (Hierarchical Features)
- 核心思想：
	- 复杂的模式可以看作是简单模式的组合。
	- 模型能够自底向上学习，从检测简单的底层特征开始，逐步构建出越来越复杂、抽象的高层特征

- 结构设计
	- 较早的卷积层通常学习检测 低级特征， 如图像边缘，线条，角点，简单纹理
	- 中间层检测，如：物体形状，轮廓 (耳朵，轮子等)
	- 深层高级特征，如 整个物体检测(猫，汽车)、场景部件(天空、草地)
	- 连续堆叠： 堆叠多个卷积层后，模型能够逐渐构建越来越复杂的特征表示
---

## 转置卷积 Transpose Convolution
- 定义
	- 通过反向操作，扩大特征图尺寸 (**上采样**)
- 核心思想
	- 通过交换卷积的前向和反向过程， 实现上采样
		- 标准卷积：输入通过滑动窗口计算输出  (可能会下采样)
		- 转置卷积：通过填充、扩展输入和卷积核的转置操作，将小尺寸输入映射到大尺寸输出
	- 🧠：普通卷积将图像的大小减小，而**转置卷积将图像大小扩大**。
	- 🌰：在图像生成，通常从一个小的潜在空间(Latent Space)开始，经过多个转置卷积层，生成更高分辨率的图像。
- 工作原理
	1. **输入扩展 (上采样) Up-Sampling**：
		- 假设你有一个大小为 $H \times W$的输入特征图
		- 使用不同的方法将图像大小扩大
	2. **填充边界**：
		- 根据设定的值扩展输入
	3. 滤波器：
		- 使用一个大小为 $K \times K$ 的滤波器。
	4. 输出特征图：经过转置卷积操作后，你将得到一个更大的输出特征图
		- 尺寸由滤波器的步长（stride）、填充（padding）和步幅决定。
- **上采样方法**：
	- 零填充 Zero Padding
		- 原理：通过在输入特征图的边缘添加0值，使得在反向卷积过程中，Filter能够产生更多的输出
		- 应用：增加图像尺寸，不影响输出特征图的尺寸。
	- 插值 Upsampling
		- 原理：根据已有像素之间的值来估算新像素值
			- 双线性插值：对周转四个像素算加权平均值
			- **双三次插值** ： 利用周围的16个像素（4×4网格）进行更复杂的加权计算
			- 最近邻插值
	- 步幅控制 Stide
		- 原理：调整Stide，控制Filter的滑动速度。
- 关键要点：
	- 上采样：图像尺寸增大
	- 可学习参数
	- 非逆运算：和数学上的反卷积需要严格求解逆操作不同，转置卷积不是真正意义上的反卷积，只是结构上的逆向。
- 典型应用场景
	- 图像上采样：
		- 图像恢复
		- 图片分割
			- 语义分割 Semantic Segmentation
		- 超分辨率任务
	- 生成对抗网络 GAN
	- 自编码器重建阶段：将低维Embedding 重新映射回原始图像的空间

---
## CNN变体
### 高级CNN架构

#### VGG Net
- 定义：
	- 简单且统一的网络架构
	- 使用多个$3*3$卷积 (使用多个堆叠的卷积基)
- 优势
	- 简洁有效， 易扩展
- 缺点：
	- 参数较多 
	- 计算成本高，效率低
- 应用：
	- 图像分类
	- 特征提取
- 实现
	- <font style="background-color: #FFFACD; color:tomato; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">VGG16</font>
		- 16个神经层：13个卷积层+3个全连接层 还有5个汇集层，1个展平层
		- [✍️ 使用VGG16微调做图标分类：决定一个图标是否是Folder Icon](https://github.com/jimmy-pink/colab-playground/blob/main/pre-trained/vgg16.ipynb)
#### RegNet
- 描述
	- 发现通用的网络设计原则，通过不同的参数化策略来定义和优化网络的设计
	- 使得网络可以随着计算资源的增加而有效地扩展。
- 优势
	- 扩展性和灵活性好
	- 性能优越
- 缺点
	- 对计算资源要求高
- 应用
	- 大规模图像分类

#### SqueezeNet
- 描述：
	- 设计了 **火焰模块(Fire Module)**, 通过减少卷积来减少参数量，同时保持较高精度
- 优势
	- 参数量小
	- 适合低算力设备
- 缺点
	- 精度差
- 应用
	- 边缘设备
	- 嵌入式设备

#### YOLO 
> (You Only Look Once)
- 描述：
	- **实时物体检测模型**
	- 在单独的网络中同时预测边界框和类标签
- 优势
	- 速度快 具有很高的处理速度，适合实时物体检测应用
	- 精度高
- 缺点：
	- 对小物体的检测精度可能较差
- 变体：
	- YOLOv3, v4, v5 都作了性能优化

### Deeper Networks
#### 残差连接 ResNet
> Residual Connections
- 定义
	- 允许信息跳跃通过多个层传递
	- 解决梯度消失问题
- 优势：
	- 通过残差连接显著提高了深度网络的性能
	- 允许**网络更深**
- 缺点
	- 较大的网络结构仍然需要很大的计算资源
- 应用：
	- 图像分类
	- 物体检测
	- 语义分割
- 变体：
	- ResNet-50/101/152
		- 不同层数的ResNet模型，适用于不同精度和算力的场景
		- [✍️ ResNet50V2动手实战：使用RestNet50V2预训练模型微调，从3.7k图标中识别出文件夹图标](https://github.com/jimmy-pink/colab-playground/blob/main/pre-trained/ResNet50V2-FolderIconRecognition.ipynb)

### 多尺度卷积  **Multi-scale Convolution**
#### Inception (GoogLeNet)
- 描述：
	- 在同一层中使用不同的大小的卷积核 进行卷积操作
	- **提取不同尺度的特征**
- 优势：
	- 通过多尺度特征提取，模型效率更高
- 缺点
	- 架构复杂，难经理解和调整
- 应用
	- 图像分类
	- 目标检测
- 变体：
	- Inception-3:
		- 有所改进。在计算机视觉任务上取得了更好的性能

#### FPN（Feature Pyramid Networks）
- 描述
	- 多尺度的特征图，结合了不同层级的特征图来进行目标检测。

#### 空洞卷积 (Dilated Convolution)
- 数学表示：$$(I *_{d} K)(i,j) = \sum_{m}\sum_{n} I(i+d \cdot m, j+d \cdot n) \cdot K(m, n)$$
	- d: 空洞率， 控制采样间隔
- 特点：
	- 扩大感受野而不增加参数量
	- 适用于需要大范围上下文的任务
- 应用：
	- WaveNet 音频生成
	- DeepLab系列

### 可变形卷积 (Deformable Convolution)
- 核心思想：
	- 让卷积核的采样位置根据输入内容动态调整
- 数学表示 $$y(p) = \sum_{k=1}^{K} w_k \cdot x(p + p_k + \Delta p_k)$$
	- $Δp_k$：通过学习得到的偏移量
- 优势：适应物体形变、姿态变化
- 应用：目标检测
#### Deformable Convolutional Networks (DCN)
- 描述
	- 让卷积核在训练过程中自适应调整其采样位置的方法
	- 让网络能够灵活对不规则形状进行处理
- 应用
	- 目标检测
	- 语义分割
- **论文**：[Deformable Convolutional Networks](https://arxiv.org/abs/1703.06211)
### 深度可分离卷积 (Depthwise Separable Convolution)
- 数学分解
	1. 逐通道卷积（Depthwise）$$\hat{G}_{k,l,m} = \sum_{i,j} K_{k,l,i,j} \cdot F_{l,m+i,n+j}$$
	2. 点卷积 (Pointwise) $$G_{k,m,n} = \sum_{l} W_{k,l} \cdot \hat{G}_{l,m,n}$$
- 参数量对比：
	- 标准卷积： $K×K×C_{in}×C_{out}$
	- 深度可分离：$K×K×C_{in} + C_{in}×C_{out}$
- 应用：
	- MobileNet，EfficientNet 等轻量级模型

#### Xception (Extreme Inception)
- 描述：
	- 基于深度可分离卷积 (Depthwise Separable Convolutions)
	- 用更少的参数实现相同的功能，提高效率
- 优点
	- 更高效的卷积，计算速度快
- 缺点
	- 需要对模型架构进行更多的调优
- 应用：
	- 图像分类
	- 物体检测
#### MobileNet
- 描述
	- 使用深度可分离卷积
	- 可传统卷积操作分解为两部分
		- 深度卷积
		- 逐点总面积
- 优势
	- 非常高效
	- 适合移动设备和边缘计算
- 应用：
	- 移动设备上的图像分类、物体检测
- 变体
	- MobileNet V2/V3: 通过引入线性瓶颈和倒残差结构进一步提高效率和性能

#### EfficientNet
- 描述：
	- 通过 **复合缩放(Compound Scaling)** 技术来同时优化网络的深度、宽度和分辩率，提高性能
- 优势 <font style="background-color: #FFFACD; color:tomato; padding: 4px; border-radius: 8px; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); font-weight: bold;">轻量</font>
	- 高效计算和内存利用
	- 能在不同平台上提供最佳性能
- 尤其适合
	- 数据量大且类别边界模糊
	- 需要稳定训练的深层网络
- 缺点
	- 结构复杂
	- 训练过程较长
- 应用
	- 图像分类
	- 目标检测
- 预训练模型
	- **EfficientNetB0 - EfficientNetB7**: 版本越高，模型容量和计算复杂度越大


## 参考资料

- [CNN in TensorFlow by Laurence](https://www.coursera.org/learn/convolutional-neural-networks-tensorflow#modules)
