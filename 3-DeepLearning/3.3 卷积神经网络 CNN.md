<h1 style=" text-align: center; font-size: 3em; font-family: 'Georgia', serif; color: #2c3e50; margin: 0.5em 0; padding: 10px 0; border-top: 4px solid #3498db; border-bottom: 4px solid #3498db; text-transform: uppercase; letter-spacing: 3px;">CNN</h1>

![cnn-diagram.png](../images/cnn-diagram.png)
- 原理：（Convolutional Neural Network, CNN）
	- 专门用于处理**网格状拓扑结构数据**， 如：
		- 图像：2D 网络像素
		- 视频： 3D风格像素 (在时间维度上)
		- 序列数据： 1D 网格(如，在NLP中，  将单词嵌入表示为2D或3D，或动态结构序列)
	- 等的深度神经网络。
	- 是一种特定类型的**深度前馈神经网络 Deep Feedforward Neural Network**
	- 通过
		- **局部感知** [Local Receptive Fields](.md#局部感知 (Local Receptive Fields))
		- **权重共享** [Shared Weights](.md#权重共享 (Shared Weights))
		- **层次化特征提取** [Hierarchical Features](.md#层次化特征 (Hierarchical Features))
	- 自动学习数据的空间或时序模式。
- 🧠 核心思想：
	- 用一组可学习的<font style="background-color:tomato; color:black">滤波器</font>在输入数据上进行 **滑动窗口操作 (Sliding Window Operation)**
	- 在每个窗口位置提取特征
	- 然后将这些特征组合起来得到最终输出
- 🌰 类比：
	- 人眼识别猫：1. 从局部细节(耳朵形状、胡须)；2. 局部特征组合，判断是否是猫
	- CNN：**从边缘→纹理→物体部件→完整物体**逐步抽象
- 🔧 核心组件：
	- 通过<font style="background-color:yellow; color:black">卷积层</font>提取**局部特征**
	- 通过<font style="background-color:yellow; color:black">池化层</font>减少**维度**
	- 最终通过<font style="background-color:yellow; color:black">全连接层</font>进行**分类或回归**
- 特点：
	- 卷积操作使得网络可以自动学习空间层次结构
	- 权重共享和局部连接有效减少参数
	- 池化层减少空间维度，从而减轻计算负担
- 缺点：
	- 对序列数据处理不如RNN
	- 不能有效处理非结构化数据
- 应用场景：
	- 图像分类
	- 目标检测
	- 人脸识别
	- 语音识别
- 入门典例：<font style="background-color:yellow; color:black">Fashion-Mnist</font>
	- [Tensorflow 入门实战#Fashion Mnist - CNN](../3-DeepLearning/Tensorflow%20%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98.md#Fashion Mnist - CNN)

### 🔧 CNN的核心组件

#### 输入层 (Input Layer)
- 功能： **接收原始数据**
	- 如：一张图像通常表示为由像素值组成的3D张量，形状为`[Height, Width, Channels]`
		- `[28,28,3]` 表示一张$28*28$的RGB图像
		- `[28, 28, 1]` 表示一张灰度图
	- 也可以将张量转为1D向量输入

#### 卷积层 (Convolutional Layer)
- 定义：
	-  卷积：给图像加滤镜以突出图像的重要标志性特征。
- 功能：
	- 用**卷积核 (滤波器) 扫描输入**，**提取局部特征**。
	- 是CNN的核心计算单元
- 卷积核(滤波器)：
	- 一个小的权重矩阵，这些权重是共享的
	- 同一个滤波器在输入数据的不同位置检测的是同一个特征，只是提取到的具体**响应强度**不同。
	- 通常卷积层会包含多个(如32，64，128)并行的滤波器，每个滤波器关注一种不同的局部特征。
- 工作流程：
	- **卷积运算**： 
		- 滤波器 在输入数据的一个 局部区域 (滑动窗口区域) 进行元素相乘
		- 将所有相乘结果相加，得到该 滤波器 在当前位置的一个输出值
	- 将滤波器 平移 到输入数据的下一个位置，重复上述过程，直到覆盖输入数据的所有区域
	- 一个滤波器在整个输入上滑动的结果形成了一个**特征图 (Feature Map)** 或 **卷积图 (Convolutional Map)**
- 例子 🌰：
	- 卷积核权重参数为：$$\begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 1 \end{bmatrix}$$
	- 计算过程![cnn-filter-workflow.png](../images/cnn-filter-workflow.png)
	- 输出：`3*3`特征图
 - 关键参数：
	- 步长 (Stide): 卷积移动的步幅，通常为1 
	- 填充 (Padding): 在图像边缘补0，控制输出尺寸。
		- 使用 same 填充通常可以保持输入和输出的空间尺寸相同
		- 使用 valid 填充则不添加填充，输出会小于输入
- 数学公式：$$(I * K)(i,j) = \sum_{m}\sum_{n} I(i+m, j+n) \cdot K(m, n)$$
	- `I`：输入矩阵（如图像像素）
	- `K`：卷积核（如3×3的权重矩阵）
	- `*`：卷积运算
	
#### 池化层/汇集层 (Polling Layer)
- 功能：
	- 下采样 (Downsampling): 
		- 将一组像素合并为一个代表值 （压缩图片）
		- 降维、减少计算量
	- 平移不变性 (Translation Invariance)：
		- 即使输入图片有小幅度变化（如平移、旋转），汇集层能保持特征不变。
		- 防止过拟合, 提高模型的泛化能力
- 工作流程：
	- 以**固定大小的矩形窗口**在特征图上滑动，并对窗口内的数值执行某个操作。
- 常见类型：
	- 最大池化 (Max Pooling)：取窗口内最大值 （保留显著特征）![cnn-maxpooling.png](../images/cnn-maxpooling.png)
	- 平均池化 (Average Pooling)：取窗口内平均值 （平滑特征）  ![cnn-avg-pooling.png](../images/cnn-avg-pooling.png)
- 超参数：
	- 池化窗口大小 (e.g., `2*2`)
	- 步长 (e.g., 2) 通常步长等于窗口大小

#### 卷积基
由卷积层、激活函数、池化层组成。

#### 激活函数 (Activation Function)
- 功能：
	- 引入非线性，使网络能拟合复杂函数
- 常用函数：
	- **ReLU**：$f(x) = \max(0, x)$（解决梯度消失，计算高效）。
	- **Leaky ReLU**：负区间保留微小梯度，避免神经元“死亡”。
	
#### 全连接层 (Fully Connected Layer)
- 功能： 
	- 将高层特征映射到最终输出 
- 工作原理：
	- 将前面卷积层和池化层提取到的 高级、抽象 的局部特征组合起来
	- 学习从 这些特征 到 最终输出 的全局映射关系 (最终决策)
- 位置：
	- 通常位于CNN末尾，连接所有神经元
- ⚠️：
	- 现代CNN，常用**全局平均池化 (GAP)** 替代全连接层以减少参数量

#### 输出层 (Output Layer)
- 位置：
	- 通常在全连接层之后
- 结构：
	- 二分类问题： 包含1个神经元，使用**Sigmoid**激活函数
	- 多分类问题：包含N个神经元(N为类别数)，使用**Softmax**激活函数
- 损失函数：
	- Binary Cross-Entropy
	- Categorical Cross-Entropy

### CNN的独特设计原则

#### 局部感知 Local Reception
- 核心思想：
	- 实际世界的信息，特别是图像，往往是 **<font style="background-color:tomato; color:black">局部相关</font>** 的，一个像素的意义很大程序上取决于 **<font style="background-color:tomato; color:black">邻域</font>** 的像素
- 优势：
	- 减少参数量
	- 捕捉局部模式，如边缘、角点
	
- <font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">局部感受野</font> (Local Receptive Fields)
	- 定义：
		- 在CNN中，某一层的输出特征图上的每个像素在输入图像上所对应的区域大小。
	- 特点：
		- 感受野的大小决定了该层的感知范围
		- CNN每一层的感受野 由前一层的卷积核尺寸、步长、填充等超参数决定。
		-  通过不断堆叠卷积层，感受野会逐渐扩大，从而能够捕捉到更大尺度的特征。
	- 感受野为什么重要？
		- 感受野的大小对卷积神经网络的性能和特征提取能力有重要影响。
			- 较小的感受野可以捕捉到细节信息，适合处理图像中的小细体或细微纹理
			- 较大的感受野可以捕捉到更大尺度的特征，适合处理图像中的大物体或全局结构
	
#### 权重共享 (Shared Weights)
- 原理：
	- 同一个卷积核在整张图上滑动，参数复用 (内部权重保持不变)
- 效果：
	- 卷积核学习到的模式 (边缘方向、纹理、形状、部件) 可以在输入数据中的任意位置被检测到。
- 优势：
	- 大幅降低计算成本，增强平移不变性。

#### 层次化特征 (Hierarchical Features)
- 核心思想：
	- 复杂的模式可以看作是简单模式的组合。
	- 模型能够自底向上学习，从检测简单的底层特征开始，逐步构建出越来越复杂、抽象的高层特征

- 结构设计
	- 较早的卷积层通常学习检测 低级特征， 如图像边缘，线条，角点，简单纹理
	- 中间层检测，如：物体形状，轮廓 (耳朵，轮子等)
	- 深层高级特征，如 整个物体检测(猫，汽车)、场景部件(天空、草地)
	- 连续堆叠： 堆叠多个卷积层后，模型能够逐渐构建越来越复杂的特征表示


### CNN的变体与扩展

#### 空洞卷积 (Dilated Convolution)
- 数学表示：$$(I *_{d} K)(i,j) = \sum_{m}\sum_{n} I(i+d \cdot m, j+d \cdot n) \cdot K(m, n)$$
	- d: 空洞率， 控制采样间隔
- 特点：
	- 扩大感受野而不增加参数量
	- 适用于需要大范围上下文的任务
- 应用：
	- WaveNet 音频生成
	- DeepLab系列

#### 可变形卷积 (Deformable Convolution)
- 核心思想：
	- 让卷积核的采样位置根据输入内容动态调整
- 数学表示 $$y(p) = \sum_{k=1}^{K} w_k \cdot x(p + p_k + \Delta p_k)$$
	- $Δp_k$：通过学习得到的偏移量
- 优势：适应物体形变、姿态变化
- 应用：目标检测

#### 深度可分离卷积 (Depthwise Separable Convolution)
- 数学分解
	1. 逐通道卷积（Depthwise）$$\hat{G}_{k,l,m} = \sum_{i,j} K_{k,l,i,j} \cdot F_{l,m+i,n+j}$$
	2. 点卷积 (Pointwise) $$G_{k,m,n} = \sum_{l} W_{k,l} \cdot \hat{G}_{l,m,n}$$
- 参数量对比：
	- 标准卷积： $K×K×C_{in}×C_{out}$
	- 深度可分离：$K×K×C_{in} + C_{in}×C_{out}$
- 应用：
	- MobileNet，EfficientNet 等轻量级模型

## 参考资料

- [CNN in TensorFlow by Laurence](https://www.coursera.org/learn/convolutional-neural-networks-tensorflow#modules)