
[Courseraè¯¾ç¨‹ï¼š DeepLearning.AI TensorFlow Developer Professional Certificate ](https://www.coursera.org/professional-certificates/tensorflow-in-practice)

[Tensorflow Keras API](https://www.tensorflow.org/api_docs/python/tf/keras/)

[ä¸»è®²è€å¸ˆåŠ³ä¼¦æ–¯çš„ä¸ªäººç½‘ç«™](https://laurencemoroney.com/about.html)

[Kaggle:æœºå™¨å­¦ä¹ èµ›äº‹å’ŒæŒ‘æˆ˜](https://www.kaggle.com/)

[Github Fashion Mnist](https://github.com/zalandoresearch/fashion-mnist)

[Corsera Official Colab](https://www.coursera.org/learn/introduction-tensorflow/ungradedLab/6Hb8q/get-hands-on-with-computer-vision-lab-1/lab?path=%2Flab%2Ftree%2Flab_1)


![ans-data-rules.png](../images/ans-data-rules.png)
å…¬å¼è¡¨ç¤ºæœºå™¨å­¦ä¹ çš„æ•°å­¦åŸç† (æ ¹æ®æ•°æ®å’Œç›®æ ‡å˜é‡Lablesï¼Œ æ¨ç†å¾—åˆ°è§„åˆ™å‡½æ•°)ï¼š
$$f(Data, Labels) = \text{Rules}$$

### ç®€å•çº¿æ€§å›å½’
<font style="background-color:tomato; color:black">å¿«é€Ÿä¸Šæ‰‹</font> è®­ç»ƒä¸€æ‰¹æ•°æ®ï¼Œä½¿å…¶æŒæ¡ y=2x+1 çš„è§„å¾‹  
`x_array = [0, 1, 2, 3, 4, 5]` ç‰¹å¾    
`y_array = [-1, 1, 3, 5, 7, 9]` æ ‡ç­¾  

#### Sequential API
ç»è¿‡ n è½®è®­ç»ƒï¼Œè®¡ç®—æœºå°†æŒæ¡ y=2+1 çš„è§„å¾‹ï¼Œç»™å‡ºä¸€äº›æµ‹è¯•å€¼$x_i$ï¼Œæ¨¡å‹å°†ç»™å‡º$y_i = 2* x_i +1$çš„è¿‘ä¼¼å€¼
```python
import tensorflow as tf
import numpy as np

# 1. æ„é€ è®­ç»ƒæ•°æ® (y = 2x - 1)
def create_training_data():

    # Define feature and target tensors with the values for houses with 1 up to 6 bedrooms.
    # For this exercise, please arrange the values in ascending order (i.e. 1, 2, 3, and so on).
    x_train = np.array([0, 1, 2, 3, 4, 5], dtype=np.float32)  # è¾“å…¥æ•°æ®
    y_train = np.array([-1, 1, 3, 5, 7, 9], dtype=np.float32)   # çœŸå®æ ‡ç­¾, å…¶å®åªæœ‰6ä¸ªæ•°[-1, 1, 3, 5, 7, 9]ï¼Œ x_trainçš„å€¼åªæœ‰å…­ä¸ª


    return x_train, x_train

def define_and_compile_model():

    # Define a compiled (but untrained) modelã€‚ å®šä¹‰æ¨¡å‹ï¼Œå°±æ˜¯å®šä¹‰æ¨¡å‹çš„ç¥ç»å›¾ç»œå±‚ã€‚
    model = tf.keras.Sequential([
        tf.keras.Input(shape=(1,)),  # è¾“å…¥å±‚ï¼š1ä¸ªè¾“å…¥ç‰¹å¾
        tf.keras.layers.Dense(units=1)  # è¾“å‡ºå±‚ï¼š1ä¸ªç¥ç»å…ƒ
    ])

    # ç¼–è¯‘æ¨¡å‹
    # sgdï¼ˆStochastic Gradient Descentï¼Œéšæœºæ¢¯åº¦ä¸‹é™ï¼‰
    # mseï¼ˆMean Squared Errorï¼Œå‡æ–¹è¯¯å·®ï¼‰ï¼Œè¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å·®è·ã€‚
    model.compile(optimizer='sgd', loss="mse")

    return model

def train_model():

    # Define feature and target tensors with the values for houses with 1 up to 6 bedrooms
    # Hint: Remember you already coded a function that does this!
    x_train, y_train = create_training_data()

    model = define_and_compile_model()

    # Train your model for 500 epochs by feeding the training data
    model.fit(x_train, y_train, epochs=500, verbose=0)

    return model


# ä¿å­˜æ¨¡å‹
model = train_model()
model.save('my_model.keras')

# åŠ è½½æ¨¡å‹
loaded_model = tf.keras.models.load_model('my_model.keras')

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
xn = np.array([6], dtype=np.float32)  # éœ€è¦é¢„æµ‹çš„ x å€¼
yn_pred = loaded_model.predict(xn)

print(f'è¾“å…¥ xn={xn[0]}ï¼Œé¢„æµ‹è¾“å‡º yn={yn_pred}')

```
### Keras å…¥é—¨
#### Functional API in Keras
```python
import numpy as np  
from tensorflow.keras.layers import Dropout, Input, Dense  
from tensorflow.keras.models import Model  
from tensorflow.keras.layers import BatchNormalization

X_train = np.random.rand(1000, 20) 
y_train = np.random.randint(2, size=(1000, 1)) 
X_test = np.random.rand(200, 20) 
y_test = np.random.randint(2, size=(200, 1)) 

input_layer = Input(shape=(20,))

hidden_layer1 = Dense(64, activation='relu')(input_layer) 
dropout1 = Dropout(0.5)(hidden_layer1)
batch_norm1 = BatchNormalization()(dropout1)

hidden_layer2 = Dense(64, activation='relu')(batch_norm1) 
dropout2 = Dropout(0.5)(hidden_layer2)
batch_norm2 = BatchNormalization()(dropout2)

output_layer = Dense(1, activation='sigmoid')(batch_norm2) 

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32) 
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test loss: {loss}')
print(f'Test accuracy: {accuracy}')
```
#### è‡ªå®šä¹‰æ¨¡å‹å±‚
```python
from tensorflow.keras.layers import Layer
from tensorflow.keras.models import Sequential

class CustomDenseLayer(Layer):
    def __init__(self, units=128):
        super(CustomDenseLayer, self).__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(shape=(input_shape[-1], self.units),
                                 initializer='random_normal',
                                 trainable=True)
        self.b = self.add_weight(shape=(self.units,),
                                 initializer='zeros',
                                 trainable=True)

    def call(self, inputs):
        return tf.nn.relu(tf.matmul(inputs, self.w) + self.b)

# Integrate the new custom layer into a model
model = Sequential([
    CustomDenseLayer(128),
    CustomDenseLayer(10)
])

# Recompile the model
model.compile(optimizer='adam', loss='categorical_crossentropy')

# Train the model again
model.fit(x_train, y_train, epochs=10, batch_size=32)
```

### Sarcasm - FNN

Sarcasm æ–°é—»æ ‡é¢˜è®½åˆºæ£€æµ‹
- é—®é¢˜åˆ†æ
	- é—®é¢˜ç±»å‹ï¼š äºŒåˆ†ç±»é—®é¢˜
	- ç›®æ ‡ï¼š
	    - è¾“å…¥ä¸€æ®µæ–‡å­—
	    - è¾“å‡ºï¼šæ£€æµ‹æ˜¯å¦è®½åˆº
	- æŒ‘æˆ˜ï¼š
	    - è®½åˆºçš„è¯­ä¹‰è¾ƒä¸ºå¤æ‚ï¼Œå¸¸å¸¸ä¾èµ–äºä¸Šä¸‹æ–‡ã€è¯­æ°”ã€æ–‡åŒ–èƒŒæ™¯ç­‰å› ç´ ï¼Œéš¾ä»¥é€šè¿‡ç®€å•çš„æ–‡æœ¬æ¨¡å¼è¿›è¡Œæ£€æµ‹
	    - è®½åˆºçš„è¯­è¨€é€šå¸¸å¸¦æœ‰éšæ™¦çš„æƒ…æ„Ÿï¼Œå¯èƒ½éœ€è¦æ¨¡å‹ç†è§£éšè—çš„æƒ…ç»ªã€‚
	    - æ•°æ®é›†å¯èƒ½æœ‰ ç±»ä¸å¹³è¡¡é—®é¢˜ï¼Œå³è®½åˆºæ–°é—»çš„æ•°é‡å¯èƒ½å°‘äºéè®½åˆºæ–°é—»ã€‚
- ç‰¹å¾å·¥ç¨‹
	- çŸ¢é‡åŒ–
	- å­è¯åˆ†è¯
- æ¨¡å‹é€‰æ‹©
	- FNN
[âœï¸ Github-NLP-Sarcasm-FNN](https://github.com/jimmy-pink/colab-playground/blob/main/coursera-lab/nlp-sarcasm-fnn.ipynb)

```python
from keras import Sequential, Input, layers 
model = Sequential([ 
	Input(shape = (MAX_LENGTH, )), 
	layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM), 
	layers.GlobalAveragePooling1D(), 
	layers.Dense(24, activation="relu"), 
	layers.Dense(1, activation="sigmoid") 
]) 
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=['accuracy'])
```

## Fashion Mnist - CNN

<font style="background-color:tomato; color:black">å¿«é€Ÿä¸Šæ‰‹</font> Fashion Mnist è®­ç»ƒæ¨¡å‹è¯†åˆ«æœè£…ç±»å‹
![fashion-mnist.png](../images/fashion-mnist.png)
1. é—®é¢˜å®šä¹‰ï¼š
	- è®­ç»ƒ7ä¸‡ä¸ªæ—¶å°šç©¿ç€ï¼Œä½¿æœºå™¨èƒ½è¯†åˆ«æ—¶è£…åˆ†ç±»
2. æ•°æ®æ”¶é›†ä¸å‡†å¤‡ï¼š
	- [Github Fashion Mnist](https://github.com/zalandoresearch/fashion-mnist)
	- *æ•°æ®æºå·²ç»æ˜¯å¹²å‡€ã€å®Œæ•´ã€æœ‰æ ‡ç­¾çš„æ•°æ®ï¼Œä¸éœ€è¦å†é¢„å¤„ç†*
	- æ•°æ®å¯è§†åŒ– ä¸ ç‰¹å¾åˆ†æ
		- matplotlab å¯è§†åŒ–(åŸå›¾ä¸º`28*28`åƒç´ ï¼Œä¸ºæ–¹ä¾¿æ˜¾ç¤ºå·²è½¬ä¸º`16*16`)
		```text
		[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
		 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
		 [  0   0   0   0   0   0   0   0   0   1   1   0   0   0   1   0]
		 [  0   0   0   0   0   0   0   0   0   0   0   3   2   2   0   0]
		 [  0   0   0   0   0   0   0   1   0  16  11   0   0   0  37   3]
		 [  0   0   0   0   0   0   0   3   2  97 111  48  41  69 102   2]
		 [  0   0   0   0   0   0   3   0  60 132 157 180 172 171 126   0]
		 [  0   0   1   1   2   3   0  40 118 110 153 144 166 155 143  11]
		 [  1   1   0   0   0   0  44 118 109 130 146 147 161 146 167  63]
		 [  0   2  18  23  54  90 116 113 128 142 156 156 152 143 165 105]
		 [ 40  81  90  99 112 120 120 119 130 145 149 154 154 144 177 116]
		 [ 87 155 148 153 139 133 141 165 179 190 191 209 247 233 231 113]
		 [  0  11  45  90 113 117 118 117  80  24  13  67 129 105 102  45]
		 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
		 [  0   1   1   3   4   4   4   4   3   1   1   2   4   3   3   2]
		 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]
		```
		
3. ç‰¹å¾å·¥ç¨‹ï¼š
	- å½’ä¸€åŒ– Normalizationï¼šå°†æ•°å€¼ä»`[0, 255]`è½¬ä¸º`[0, 1]`
4. æ¨¡å‹é€‰æ‹©ï¼š
	- å·ç§¯ç¥ç»ç½‘ç»œ CNN
		- è¾“å…¥å±‚ï¼š28åƒç´ å›¾ç‰‡
		- å·ç§¯åŸºï¼š
			- å·ç§¯å±‚ï¼š`tf.keras.layers.Conv2D`
			- æ± åŒ–å±‚ï¼š`tf.keras.layers.MaxPooling2D`
		- å±•å¼€ Flatternï¼š å°†`28*28`å›¾ç‰‡åƒç´ çŸ©é˜µè½¬1ç»´å‘é‡
			- ç›®çš„ï¼šè¿æ¥å…¨è¿æ¥å±‚ã€‚å…¨è¿æ¥å±‚åœ¨ç¥ç»ç½‘ç»œä¸­é€šå¸¸è¦æ±‚è¾“å…¥1ç»´å‘é‡
		- ReLUå¯†é›†å±‚ï¼š`tf.keras.layers.Dense(128, activation='relu')`
			- å¯¹å·ç§¯åŸºæä¾›çš„ç‰¹å¾è¿›ä¸€æ­¥å¤„ç†å’Œæ˜ å°„
		- è¾“å‡ºå±‚ï¼š`Â tf.keras.layers.Dense(10, activation='softmax')`
			- æ¯ä¸ªç±»åˆ«çš„probability
5. æ¨¡å‹è®­ç»ƒ
6. æ¨¡å‹è¯„ä¼°

```python
import tensorflow as tf 
import numpy as np 

# Load the Fashion MNIST dataset 
fmnist = tf.keras.datasets.fashion_mnist 
(training_images, training_labels), (test_images, test_labels) = fmnist.load_data() 
# å½’ä¸€åŒ–ã€‚Normalize the pixel values 
training_images = training_images / 255.0 
test_images = test_images / 255.0 
#å°†åŸæœ¬å½¢çŠ¶ä¸º (28, 28) çš„å›¾åƒæ•°ç»„é‡å¡‘ä¸º (28, 28, 1)ï¼Œå°†æ¯ä¸ªå›¾åƒçœ‹ä½œä¸€ä¸ªç°åº¦å›¾ã€‚ 
training_images = np.expand_dims(training_images, axis=-1) 
# æ·»åŠ ä¸€ä¸ªå•ä¸€å·ç§¯å±‚å’Œæ±‡é›†å±‚ï¼Œæé«˜é¢„æµ‹çš„å‡†ç¡®ç‡ 
model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(28,28,1)),
    # åˆ›å»º64ä¸ªfilters(æ»¤æ³¢å™¨ï¼Œä¹Ÿå°±æ˜¯å·ç§¯æ ¸),è¿™äº›æ»¤é•œæ˜¯3*3(ä¹Ÿå°±æ˜¯æ£€æŸ¥æ¯ä¸ªåƒç´ æ ¼3*3çš„åƒç´ )
    # å·ç§¯ä¸ä¼šå‹ç¼©å›¾åƒï¼Œå·ç§¯è¿‡åæ¯ä¸ªåƒç´ æ ¼ä¸Šçš„æ•°å­—éƒ½ä¼šè¢«filteré‡æ–°è®¡ç®—ï¼Œè¾“å‡ºçš„å›¾åƒæ˜¯26*26
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    # æœ€å¤§æ± ï¼Œæ¯4ä¸ªåƒç´ æ ¼ä¸­æœ€å¤§çš„å€¼å­˜æ´»ï¼›ä¼šå°†å›¾åƒå‹ç¼©åˆ°åˆ†è¾¨ç‡å‡åŠã€‚
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
# Print the model summary æ‰“å°æ¨¡å‹ç»“æ„ä¿¡æ¯
model.summary()
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
# Train the model
model.fit(training_images, training_labels, epochs=5)

# è¯„ä¼°æ¨¡å‹çš„æŸå¤±å€¼å’Œç²¾ç¡®åº¦ ä¸ä¼šåè®­æ¨¡å‹
test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)
```

## IMDB_Reviews å½±è¯„æƒ…æ„Ÿåˆ†æ

-  é—®é¢˜åˆ†æ
	
	- é—®é¢˜ç±»å‹ï¼š äºŒåˆ†ç±»é—®é¢˜
	- ç›®æ ‡ï¼š
	    - è¾“å…¥: æ¯æ¡æ•°æ®æ˜¯ä¸€æ¡å½±è¯„ï¼ˆæ–‡æœ¬ï¼‰ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªè¯è¯­
	    - è¾“å‡ºï¼šæƒ…æ„Ÿæ ‡ç­¾ï¼š1è¡¨ç¤ºæ­£é¢è¯„ä»·ï¼Œ0è¡¨ç¤ºè´Ÿé¢è¯„ä»·ã€‚
	- æŒ‘æˆ˜ï¼š
		- **æ•°æ®å™ªå£°**ï¼šå½±è¯„çš„å†…å®¹å¾€å¾€æ¯”è¾ƒå£è¯­åŒ–ï¼ŒåŒ…å«å¤šç§è¡¨è¿°æ–¹å¼ã€‚
		    
		- **æ–‡æœ¬çš„é•¿çŸ­ä¸ä¸€**ï¼šæ¯æ¡å½±è¯„çš„é•¿åº¦å¯èƒ½å·®å¼‚å¾ˆå¤§ï¼Œæœ‰çš„éå¸¸ç®€çŸ­ï¼Œæœ‰çš„åˆ™å¾ˆé•¿ã€‚
		    
		- **è¯æ±‡é—®é¢˜**ï¼šå•è¯çš„å¤šæ ·æ€§ï¼Œè¯å½¢å˜åŒ–ï¼ˆä¾‹å¦‚å•å¤æ•°ã€åŠ¨è¯æ—¶æ€å˜åŒ–ï¼‰ç­‰ã€‚
- ç‰¹å¾å·¥ç¨‹
	- æ–‡æœ¬é¢„å¤„ç†ï¼š ä¸éœ€è¦
	- æ–‡æœ¬è¡¨ç¤º
	    - å­è¯åˆ†è¯ï¼šåŠ è½½ç°æˆå­è¯è¯åº“
	    - æ–‡æœ¬çŸ¢é‡åŒ–
	        - TF-IDFï¼ˆTerm Frequency-Inverse Document Frequencyï¼‰
	        - è¯åµŒå…¥ Word Embedding
	        - æ–‡æœ¬åºåˆ—ç¼–ç  TextVectorization
	- åºåˆ—é•¿åº¦å¤„ç†
		- ç”±äºå½±è¯„çš„é•¿åº¦ä¸ä¸€ï¼Œéœ€è¦å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œ **ç»Ÿä¸€é•¿åº¦å¤„ç†**ï¼Œé€šå¸¸åšæ³•æ˜¯ï¼š
	        - **æˆªæ–­ï¼ˆTruncatingï¼‰**ï¼šå¯¹é•¿äºè®¾å®šé•¿åº¦çš„æ–‡æœ¬è¿›è¡Œæˆªæ–­ã€‚
	        - **å¡«å……ï¼ˆPaddingï¼‰**ï¼šå¯¹çŸ­äºè®¾å®šé•¿åº¦çš„æ–‡æœ¬è¿›è¡Œå¡«å……ï¼Œå¸¸ä½¿ç”¨0å¡«å……ã€‚
- æ¨¡å‹é€‰æ‹©
	- åŒå±‚LSTM

ğŸŒ°Â **imdb_reviews å½±è¯„æƒ…æ„Ÿåˆ†æ**  
[âœï¸ Github-imdb_reviews-rnn](https://github.com/jimmy-pink/colab-machinelearning-playground/blob/main/tensorflow/nlp-imdb_reviews-rnn.ipynb)

```python
# Build the model
model = tf.keras.Sequential([
    tf.keras.Input(shape=(None,)),
    tf.keras.layers.Embedding(subword_tokenizer.vocabulary_size(), 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
```

