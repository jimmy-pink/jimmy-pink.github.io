![[transformer-full.png]]
## Transformer

### 什么是Transformer？
- 定义：
	- 一种用于处理==序列数据==的DL模型
	- 由Vaswani等人在2017年的论文《[Attention Is All You need](https://arxiv.org/abs/1706.03762)》提出
	- 通过 **自注意力机制（Self-Attention）** 来捕捉序列中不同元素（如单词、字符、特征等）之间的相互关系和上下文依赖性。
- 特点：
	- 完全依赖==注意力机制== (Attention Mechanism) 来捕捉输入序列和输出序列之间的依赖关系
- 优点：
	- 并行性： 与RNN不同，Transformer可以并行处理输入序列的所有token, 训练速度更快
	- 长距离依赖： 自注意力机制能够捕捉长距离的依赖关系，而RNN在处理长序列时容易出现梯度消失和信息丢失的问题
	- 强大的表示能力： Transformer在许多任务上取得了超越传统模型的性能
- 缺点：
	- 计算复杂度高
	- 内在消耗大
- 应用：
	- 机器翻译
	- 文本分类
	- 情感分析
	- 问答系统
	- 图像分类/Text-to-Image
	- 时间序列预测
- 变体：
	- GPT
	- BERT
	- Adobe Photoshop Image Transformer
	
### 核心组件
#### 输入嵌入 (Input Embedding)    
- 将输入的词汇通过[[4.1 NLP 入门#单词嵌入 Word Embedding|Word Embedding]]转化为高维向量。每个词token映射成一个n维向量。
![[word-embedding-1.png]]

#### 位置编码 (Positional Encoding)
![[transformer-positional-embedding.png]]
由于Transformer 不像RNN或CNN那样具有自然的时间或顺序信息，因此需要附加位置信息。  
位置编码 如何计算得到？ 
- 位置编码通常采用正弦和余弦函数。
	- 基于正弦的位置编码 (奇数维)，公式：$PE(i, 2k) = \sin\left( \frac{i}{10000^{2k/d_{\text{emb}}}} \right)$
	- 基于余弦的位置编码 (偶数维)，公式：$PE(i, 2k+1) = \cos\left( \frac{i}{10000^{2k/d_{\text{emb}}}} \right)$
		- $d_{\text{emb}}$ 是input embedding的嵌入维度
		- i 是词在输入序列中的位置，通常从0开始
		- k 是位置编码向量的维度索引。 2k和 2k+1 分别对应位置编码的 奇数维 和 偶数维
	
#### 自注意力机制 Self-Attention Mechanism
作用：
- ==计算每个token与其他所有token的相关性== 
- 捕捉序列内的全局依赖关系。  
**自注意力机制的输出通过多头注意力（Multi-Head Attention）进一步提升，将不同的相关性信息分开处理再合并。**


<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">注意力的计算步骤</font>：![[self-attention-mechanism.png]]
1. 输入表示：
	- 假设有一个输入序列X，它的维度是$(n,d)$
		- n是序列长度.  在nlp中，n一般表示输入句子的单词数，[[4.1 NLP 入门#单词嵌入 Word Embedding]]
		- d是每个元素的维度 (通常是词嵌入的维度, 上图中是5维)
	- 输入经过线性变换，得到查询(Query)，键(Key)， 值(Value)向量，公式：$$Q = X W_Q, \quad K = X W_K, \quad V = X W_V$$
		- 其中，$W_Q, W_K, W_V$ 是学习到的权重矩阵 ； 
2. 计算Attention  Scores
	- Attention Scores衡量的是查询向量(Query ) 与键向量(Key) 之间的相似度。
	- 常用的 **点积** 来计算相似度$$\text{Attention Scores} = Q K^T$$
		-  $K^T$ 是 **键（Key）** 矩阵的转置，表示查询与键之间的相似度。
3. 缩放 (Scaling)
	- 为了避免点积结果过大而导致梯度消失或爆炸，通常会对点积结果进行缩放。
	- 具体做法是将点积结果除以一个缩放因子：$$\text{Scaled Attention Scores} = \frac{Q K^T}{\sqrt{d_k}}$$
		- 其中，$d_k$ 是键向量的维度。这个缩放因子确保了点积不会变得过大。
4. 应用 Softmax：
	- 将缩放后的Attention Socres输入到Softmax函数中，以便将它们转化为概率分布。
	- 目的：将Attention Scores转换为加权系数。公式：$$\text{Attention Weights} = \text{Softmax}(\frac{Q K^T}{\sqrt{d_k}})$$
5. 加权求和 Weighted Sum：
	- 最后，使用Attention Weights 对值(Value) 向量进行加权求和，从而得到最终的Attention Output。公式：$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
		- 其中：
			- Q（Query）代表查询向量。
			- K（Key）代表键向量。
			- V（Value）代表值向量。
			- $\text{softmax}$是归一化函数。
			- $d_k$ 是键向量的维度。

#### 前馈神经网络 FFN
> Feed-Forward Network， 它和 FNN (Feed-Forward Neural Network) 是同一个东西
![[multi-layer-fnn.png]]
- 在Transformer中的作用：
	- 对每个位置的表示 进行非线性变换
	- 进一步增强模型的表达能力
	- 位置独立
		- 前馈神经网络是 **位置无关** 的，即它对输入序列中的每个位置单独进行计算
- 在Transformer中的计算流程：
	1. 输入： 经过自注意力机制后，得到的每个位置的向量表示x, （形状为 $(L, d_{\text{emb}})$）。
	2. **第一层**：将输入通过一个全连接层，将维度扩展到 $d_{\text{ff}}$（例如 2048）。
	3. **激活函数**：通常使用 ReLU 激活函数。
	4. **第二层**：通过第二个全连接层将维度压缩回 $d_{\text{emb}}$。
	5. **输出**：得到的每个位置的向量 $y_3$，它仍然是一个形状为 $(L, d_{\text{emb}})$ 的矩阵。
- 特点：
	- 用于进一步处理经过自注意力机制处理的信息
	- 通常会包含两个线性变换 和 激活函数(ReLU)
#### 残差连接 Residual Connection
- 定义 ：（图中的Add就代表RC）
	- 用来帮助信息在网络中更容易流动。
- 原理：
	- 将输入直接加到输出中，而不是仅仅依赖于网络的计算结果。
- 公式：$$\text{Output} = \text{Layer Output} + \text{Input}$$
- 目的：
	- 缓解梯度消失问题
		- 在DNN中，梯度在反向传播时可能会逐渐消失。
		- 通过残差连接，梯度可以直接传递回去，避免这个问题
	- 加速收敛
#### 层归一化 Layer Normalization
- 定义：（图中的Norm代表归一化）
	- 将每一层的输入进行标准化处理，使得每个输入的均值上为0，方差为1.
- 目的：
	- 提高训练的稳定性
		- 防止某些层的激活值过大或过小
	- 加速收敛
		- 它让每一层的输入分布保持一致，避免了训练中出现梯度爆炸和梯度消失问题
	
	
### 工作流程 

Transformer的工作流程可以分为两个主要阶段：
#### 编码阶段 Encoder
- 接收输入序列，通过自注意力机制和FFN提取输入序列的表示
- 可以被堆叠多个相同的层，形成一个Encoder Stack (一般是6层)
#### 解码阶段 Decoder
- 接收目标序列，生成输出序列。通过以下机制实现：
	- 自注意力机制：用于关注输入序列
	- 自回归注意力机制： 用于关注已生成输出序列
- Decoder 也可以被堆叠多个相同的层，形成一个Decoder Stack。
- 教师强制 Teacher Forcing
	- 在解码器中使用已知的目标序列（而不是模型预测的目标）
- ==Outputs Embedding ==
	- 将 目标序列 转换成 嵌入向量
		- 在NLP中，目标变量可能是Token (猜下一个word)，也可能是序列 (猜下一句话)
	- 解码器输入的一部分
- ==目标序列右移一位 outputs (shifted right)==
	- 原理：将目标序列右移一位
	- 目的：
		- 确保解码器只能访问前一个时间步的输出，将目标序列中的单词右移一位，然后输入解码器
		- 这样解码器在第t步时，接收到的每t-1步的输出作为输入，而t=0时输入则是`<SOS>`
	- 🌰 例子
```
		# Transformer架构的翻译任务： 输入德语 输出英文
		Input Sequence = [Le, chat, est, assis, sur, le, tapis] # 输入序列 德语
		Target Sequence = [The, cat, sat, on, the, mat] # 目标序列
		Shifted Target = [<SOS>, The, cat, sat, on, the]  # outputs (shifted right)
```
	

这两个阶段的结构和流程是高度并行化的，基于 **自注意力机制** 和 **位置编码** 的计算来进行上下文信息的传递。
每个编解码器都有若干相似组件，核心部分就是 
- 自注意力机制 
- FFN

<font style="background-color:tomato; color:white; padding:4px; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);font-weight: bold;">Transformer工作流程</font>：
![[transformer-full.png]]
1. 输入
	- **Input Embedding**
	- **Positional Encoding**
2. Encoder
	- 接收输入序列
	- 组件
		- Add & Norm, [[#残差连接 Residual Connection|残差连接]]和[[#层归一化 Layer Normalization|归一化]]
		- [[#自注意力机制 Self-Attention Mechanism|自注意力机制计算]]
		- [[#前馈神经网络 FFN]]
	- 输出：生成一个包含输入序列上下文信息的表示
		- 每个编码器层的输出都会被传递到下一层
3. Decoder
	- 接收输入，包括：
		- 来自Encoder的上下文表示
		- Shifted Target：目标序列的右移版本
	- 组件
		- 目标序列输入 Target Embedding
			- 目标序列会通过词嵌入转化为向量表示。
		- 自注意力机制
		- 编码器-解码器注意力（Encoder-Decoder Attention）机制
		- FFN
		- Add & Norm
	- 输出：生成目标序列的输出。
	
## 参考资料
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [NVIDA Blog: What’s a Transformer Model?](https://blogs.nvidia.com/blog/what-is-a-transformer-model/)
- [知乎：Transformer模型详解](https://zhuanlan.zhihu.com/p/338817680)
