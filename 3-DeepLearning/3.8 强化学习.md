<h1 style=" text-align: center; font-size: 3em; font-family: 'Georgia', serif; color: #2c3e50; margin: 0.5em 0; padding: 10px 0; border-top: 4px solid #3498db; border-bottom: 4px solid #3498db; text-transform: uppercase; letter-spacing: 3px;">Reinforcement Learning</h1>

### 核心概念
![[reinforcement-learning 1.png]]
- 什么是强化学习 Reinforcement Learning？
	- 让智能体 与 环境交互
	- 不断试错来学习如何决策
	- 以最大化 长期奖励 
- 应用场景
	- 游戏 (AlphaGo, Atari, Dota, 星际争霸)
	- 机器人：机械臂控制、步行机器人
	- 自动驾驶：决策规划系统
	- 金融投资：量化策略优化
	- 推荐系统 ： 动态个性化推荐


### 主要元素
- 智能体 Agent
	- 学习做决策的主体， 如机器人，游戏玩家
- State 状态
	- 当前环境的情况描述，如棋盘布局，游戏画面
	- 状态可以是数据，也可以是向量(如速度)、矩阵(如游戏画面)、张量。
- 动作 Action
	- Agent 在某一状态下可以选择的行为
- 奖励 Reward
	- 环境给Agent的反馈，用来评估行动好坏
- 策略 Policy
	- 决定Agent如何在每个状态下选择动作的规则
- 价值函数 Value Function
	- 衡量一个状态或状态-动作组合的长期奖励预期
- 回合 Episode 
	- 从初始状态开始到结束状态的一段交互过程。


### 学习流程
1. Agent 观察当前状态 $s_t$
2. 根据策略 $\pi(s_t)$ 选择一个动作$a_t$
3. 执行动作$a_t$，环境转移到新状态$s_{t+1}$，并给予奖励$r_t$
4. Agent根据奖励和新状态来更新策略
5. 重复以上步骤，直到学会最优策略


### 三大核心函数
- 状态价值函数 $V^π(s)$
	- 在状态s下遵循策略π的预期回报
- 动作价值函数 $Q^π(s,a)$
	- 在状态s采取动作a后遵循π的预期回报
- 优势函数 $A^π(s,a)$
	- $Q^π(s,a)$ - $V^π(s)$ → 动作相对优势

### 两大基础方法
- 基于价值的方法
	- 代表算法： Q-Learning, DQN
	- 特点：学习价值函数 -> 推导最优策略
	- 局限： 不适合连续动作空间
- 基于策略的方法
	- 代表算法：Reinforce， pro
	- 特点：直接优化策略参数
	- 优势：处理连续动作更高效
### 强化学习的经典算法
- 时序差分法
	- SARSA (On-policy)
	- Q-learning (Off-policy)
- 深度强化学习
	- DQN (经验回放+目标网络)
	- DDPG (连续控制)
	- SAC (随机策略优化)
- 策略优化
	- TRPO (信赖域)
	- PPO (简化实现)

#### Q-Learning
[Deep Reinforcement Learning: Guide to Deep Q-Learning](https://blog.mlq.ai/deep-reinforcement-learning-q-learning/)
![[q-learning.png]]
- 定义
	- 基于价值的强化学习算法
	- Agent 与 环境互动
	- 为Agent的行为学习一个策略
	- 随着时间推移，最大化累计奖励
- 特点
	- 训练Agents来制定决策序列
	- 在一个状态中学习特定行为的价值
	- 最优动作段策略
- 工作流程
	1. 初始化环境和参数
		- 定义一个平台，比如OpenAI's Gym
		- 初始化 Q-table
			- Q表是表格结构(矩阵)，用于记录所有状态-动作对 对应的Q值$Q(s, a)$
		- 设置超参数
			- 学习率
			- discount factor
			- exploration rate
	2. 创建Q-network
		- 构建一个近似q值函数的q-network
			- Input layer size = state size
			- Output  layer size = action size
			- 2-3 hidden layers with ReLU activation
	3. 训练 Q-network
		1. 初始化状态
		2. 选择动作，使用Epsilon贪婪策略来平衡探索和开发。
		3. 执行动作 
		4. 更新Q-values
			- 使用Bellman等式
			- 计算目标Q-Value
			- 训练Q-network来最小化预测值和目标Q-value的差异
		5. 重复上述过程：减少exploration rate 来从探索切换到开发
	4. 评估Agent： 测试agent来最大化奖励
		- 使用学习策略与环境交互
		- 对已学习Q-values的利用来最大化奖励
		- 几个episodes奖励的总奖励积累来衡量
- 优点
	- 表格型的强化学习算法，适用于小状态空间
	
	

🌰 Q表：

|      | **动作 A** | **动作 B** | **动作 C** |
| ---- | -------- | -------- | -------- |
| 状态 1 | 1.2      | 0.5      | -0.3     |
| 状态 2 | 0.7      | 1.0      | 0.1      |
| 状态 3 | 0.0      | -1.2     | 0.8      |
| 状态 4 | 0.4      | 0.6      | 0.9      |
#### DQNs
> **Deep Q-Networks** or **Deep Q-Learning** 
![[deep-q-learning.png]]
- 定义
	- 当状态空间非常大时，无法使用Q表，也不再存储s,a对的Q值了
	- 而是使用**神经网络近似Q函数** $Q(s, a; \theta) \approx Q^*(s, a)$
		- Q(s, a; θ) 是神经网络的输出（给定状态 s 和动作 a）
		- θ 是神经网络的参数（就是权重和偏置）
	- 也就是说使用**神经网络来预测**某个状态下各个动作的Q值
- 目标
	- 学习一个Q函数，也就是学习每个状态-动作对的期望回报值，从而决定最优行为策略
- 关键技巧
	- 经验回收 Replay Buffer
		- 打乱时间相关性，提升训练稳定性
	- 目标网络 Taget Network
		- 使用另一个固定参数的网络，减少不稳定
	- 神经网络输入状态微量 / 图像
		- 输出多个动作的Q值
- 工作流程图解
```text
Agent与环境交互：
    观察 (s, a, r, s')
         ↓
    存入经验池（Replay Buffer）
         ↓
每隔一段时间：
    从经验池中随机抽出 batch（比如32条）
         ↓
    用这些数据训练神经网络，更新Q函数
```

### 关键实现技术
- 探索机制
	- ε-greedy
	- 噪声探索(OU过程)
	- 内在好奇心
- 稳定训练技巧
	- 目标网络冻结
	- 奖励裁剪
	- 梯度裁剪

### 前沿发展方向

- 多智能体系统
	- 竞争环境
	- 协作机制
- 分层强化学习
	- 技能自动发现
	- 时序抽象
- 元强化学习
	- 快速适应新任务
- 基于模型的方法
	- 世界模型学习